{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831db562-1e75-4dc7-9ec8-a7bcfb22f9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 merges: t,h,e, ,q,u,i,c,k, ,b,r,o,w,n, ,f,o,x, ,j,u,m,p,e,d, ,o,v,e,r, ,t,h,e, ,a,n,g,r,y, ,d,o,g\n",
      "after 5 merges: the qu,i,c,k, ,b,r,o,w,n, ,f,o,x, ,j,u,m,p,e,d, ,o,v,e,r, ,the ,a,n,g,r,y, ,d,o,g\n",
      "after 10 merges: the quick b,r,o,w,n, ,f,o,x, ,j,u,m,p,e,d, ,o,v,e,r, ,the ,a,n,g,r,y, ,d,o,g\n",
      "after 15 merges: the quick brown ,f,o,x, ,j,u,m,p,e,d, ,o,v,e,r, ,the ,a,n,g,r,y, ,d,o,g\n",
      "after 20 merges: the quick brown fox j,u,m,p,e,d, ,o,v,e,r, ,the ,a,n,g,r,y, ,d,o,g\n",
      "after 25 merges: the quick brown fox jumped, ,o,v,e,r, ,the ,a,n,g,r,y, ,d,o,g\n",
      "after 30 merges: the quick brown fox jumped over, ,the ,a,n,g,r,y, ,d,o,g\n",
      "after 35 merges: the quick brown fox jumped over the ang,r,y, ,d,o,g\n",
      "after 40 merges: the quick brown fox jumped over the angry do,g\n",
      "after 45 merges: the quick brown fox jumped over the angry dog\n"
     ]
    }
   ],
   "source": [
    "def most_common_pair(nums):\n",
    "    from collections import Counter\n",
    "    ctr = Counter(zip(nums, nums[1:]))\n",
    "    return None if len(ctr) == 0 else ctr.most_common(1)[0][0]\n",
    "\n",
    "def merge_pair(nums, pair, idx):\n",
    "    new_nums = []\n",
    "    i = 0\n",
    "    while i < len(nums):\n",
    "        if i < len(nums) - 1 and nums[i] == pair[0] and nums[i + 1] == pair[1]:\n",
    "            new_nums.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_nums.append(nums[i])\n",
    "            i += 1\n",
    "    return new_nums\n",
    "\n",
    "def get_tokenizer(text, vocab_size, start_idx=256):\n",
    "    nums = [ord(char) for char in text]\n",
    "    itos = {n: chr(n) for n in nums}\n",
    "    n_merges = start_idx - vocab_size\n",
    "    merges = {}\n",
    "    for i in range(n_merges):\n",
    "        pair = most_common_pair(nums)\n",
    "        if pair is None:\n",
    "            break\n",
    "        pair_idx = start_idx + i\n",
    "        nums = merge_pair(nums, pair, pair_idx)\n",
    "        itos[pair_idx] = itos[pair[0]] + itos[pair[1]]\n",
    "        merges[pair] = pair_idx\n",
    "    return merges, itos, nums\n",
    "\n",
    "text = 'the quick brown fox jumped over the angry dog'\n",
    "for n_merges in range(50):\n",
    "    merges, itos, nums = get_tokenizer(text, 256 - n_merges)\n",
    "    if n_merges % 5 == 0:\n",
    "        print(f'after {n_merges} merges: {\",\".join([itos[n] for n in nums])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23298497-8970-40d1-af45-3e292884141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=100, max_char_idx=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_char_idx = max_char_idx\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(self.max_char_idx)}\n",
    "        self.merged_pairs = {}\n",
    "\n",
    "    def fit(self, X_text):\n",
    "        all_text = '\\n'.join(X_text)\n",
    "        nums = self._encode(text)\n",
    "        n_merges = self.max_char_idx - self.vocab_size\n",
    "        for i in range(n_merges):\n",
    "            counts = self._get_pair_counts(nums)\n",
    "            pair = max(counts, key=counts.get)\n",
    "            pair_idx = self.max_char_idx + i\n",
    "            nums = self._merge_pair(nums, pair, pair_idx)\n",
    "            self.vocab[pair_idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            self.merged_pairs[pair] = pair_idx\n",
    "\n",
    "    def transform(self, X_text):\n",
    "        X_nums = []\n",
    "        for text in X_text:\n",
    "            nums = self._encode(text)\n",
    "            X_nums.append(nums)\n",
    "        return X_nums\n",
    "\n",
    "    def inverse_transform(self, X_nums):\n",
    "        X_text = []\n",
    "        for nums in X_nums:\n",
    "            text = self._decode(nums)\n",
    "            X_text.append(text)\n",
    "        return X_text\n",
    "\n",
    "    def nums_to_tokens(self, X_nums):\n",
    "        X_tokens = []\n",
    "        for nums in X_nums:\n",
    "            tokens = [self.vocab[idx].decode('utf-8', errors='replace') for idx in nums]\n",
    "            X_tokens.append(tokens)\n",
    "        return X_tokens\n",
    "\n",
    "    def _merge_pair(self, nums, pair, pair_idx):\n",
    "        nums_merged = []\n",
    "        i = 0\n",
    "        while i < len(nums):\n",
    "            if i < len(nums) - 1 and nums[i] == pair[0] and nums[i + 1] == pair[1]:\n",
    "                nums_merged.append(pair_idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                nums_merged.append(nums[i])\n",
    "                i += 1\n",
    "        return nums_merged\n",
    "\n",
    "    def _encode(self, text):\n",
    "        nums = list(text.encode('utf-8'))\n",
    "        while len(nums) >= 2:\n",
    "            counts = self._get_pair_counts(nums)\n",
    "            pair = min(counts, key=lambda pair: self.merged_pairs.get(pair, float('inf')))\n",
    "            if pair not in self.merged_pairs:\n",
    "                break\n",
    "            pair_idx = self.merged_pairs[pair]\n",
    "            nums = self._merge_pair(nums, pair, pair_idx)\n",
    "        return nums\n",
    "\n",
    "    def _decode(self, nums):\n",
    "        tokens = b''.join(self.vocab[idx] for idx in nums)\n",
    "        text = tokens.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_pair_counts(nums):\n",
    "        counts = {}\n",
    "        for pair in zip(nums, nums[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "text = 'the quick brown fox jumped over the angry dog'\n",
    "tokenizer = BPETokenizer(vocab_size=256 - 10)\n",
    "tokenizer.fit([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fdd764-3fdc-4b55-9632-e03211185221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[265, 114, 111, 119, 110, 32, 102, 111, 120, 32, 106, 117, 109, 112, 101, 100, 32, 111, 118, 101, 114, 32, 258, 97, 110, 103, 114, 121, 32, 100, 111, 103]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.transform([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8261e820-7c7c-44c1-9093-539db3b4b7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick b,r,o,w,n, ,f,o,x, ,j,u,m,p,e,d, ,o,v,e,r, ,the ,a,n,g,r,y, ,d,o,g\n"
     ]
    }
   ],
   "source": [
    "print(','.join(tokenizer.nums_to_tokens(tokenizer.transform([text]))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc33f26-08f5-4097-a1c3-64a21ad992ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h,e,l,l,o, ,t,o, ,the ,w,o,r,l,d,!\n"
     ]
    }
   ],
   "source": [
    "print(','.join(tokenizer.nums_to_tokens(tokenizer.transform(['hello to the world!']))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f9b60-91d1-47f4-a52c-39fba0c0ebfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
