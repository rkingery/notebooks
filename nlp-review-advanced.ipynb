{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd8cf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3763e32830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "\n",
    "import timm\n",
    "import fastai.text.all as fastai\n",
    "from fastai.callback.schedule import Learner\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed + 1)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c2b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3bfed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Path.cwd() / 'models' / 'checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c0156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, optimizer='sgd', lr=0.001, weight_decay=0, momentum=0, betas=(0.9, 0.999), eps=1e-8):\n",
    "    if optimizer == 'sgd':\n",
    "        opt = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay, \n",
    "            momentum=momentum\n",
    "        )\n",
    "    if optimizer == 'adam':\n",
    "        opt = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=betas,\n",
    "            eps=eps\n",
    "        )\n",
    "    return opt\n",
    "\n",
    "def train_classifier(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "                     tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "                     print_every=1, n_grad_accums=1, use_multi_gpus=False):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    if use_multi_gpus:\n",
    "        orig_model = model\n",
    "        model = nn.DataParallel(model)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        batch_correct = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        for i, (X, y) in enumerate(iterator):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat, y).mean() # mean is for multi-gpu losses, avg them together\n",
    "            loss /= n_grad_accums # rescale loss for grad accumulation\n",
    "            loss.backward()\n",
    "            if i % n_grad_accums == 0: # only step when every n_grad_accums grad updates\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "            batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        train_acc = sum(batch_correct) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            opt.zero_grad()\n",
    "            batch_losses = []\n",
    "            batch_correct = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, y in iterator:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                yhat = model(X)\n",
    "                loss = loss_fn(yhat, y).mean()\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "                batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "            test_acc = sum(batch_correct) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            test_acc = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Training Accuracy\", train_acc, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Accuracy\", test_acc, epoch+1)\n",
    "        if print_stats and epoch % print_every == 0:\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   train acc: {round(train_acc, 4): <6}   ' \n",
    "            s3 = f'test loss: {round(test_loss, 4): <6}   test acc: {round(test_acc, 4): <6}'\n",
    "            print(s1 + s2 + s3)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    model = model if not use_multi_gpus else orig_model\n",
    "    return model\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119e47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_from_iter(data_iter):\n",
    "#     corpus = []\n",
    "#     for text in data_iter:\n",
    "#         corpus.append(text)\n",
    "#     return corpus\n",
    "\n",
    "# train_iter = torchtext.datasets.WikiText103(split='train')\n",
    "# val_iter = torchtext.datasets.WikiText103(split='valid')\n",
    "# test_iter = torchtext.datasets.WikiText103(split='test')\n",
    "\n",
    "# train_text = text_from_iter(train_iter) + text_from_iter(val_iter)\n",
    "# test_text = text_from_iter(test_iter)\n",
    "\n",
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# train_tokens = (tokenizer(doc) for doc in tqdm(train_text))\n",
    "# test_tokens = (tokenizer(doc) for doc in tqdm(test_text))\n",
    "\n",
    "# vocab = torchtext.vocab.build_vocab_from_iterator(train_tokens, max_tokens=10000, specials=['<unk>', '<pad>'], min_freq=50)\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# stoi = vocab.get_stoi()\n",
    "# itos = vocab.get_itos()\n",
    "\n",
    "# max_tokens = 256\n",
    "# train_tokens = [doc[:max_tokens] for doc in train_tokens if len(doc) > 5]\n",
    "# test_tokens = [doc[:max_tokens] for doc in test_tokens if len(doc) > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5d3e2",
   "metadata": {},
   "source": [
    "# Machine Translation: English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb20639",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "tokenizer_en = torchtext.data.utils.get_tokenizer(nlp_en)\n",
    "tokenizer_fr = torchtext.data.utils.get_tokenizer(nlp_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d529fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135842,\n",
       " [['Go.', 'Va !'],\n",
       "  ['Run!', 'Cours!'],\n",
       "  ['Run!', 'Courez!'],\n",
       "  ['Wow!', 'Ça alors!'],\n",
       "  ['Fire!', 'Au feu !'],\n",
       "  ['Help!', \"À l'aide!\"],\n",
       "  ['Jump.', 'Saute.'],\n",
       "  ['Stop!', 'Ça suffit!'],\n",
       "  ['Stop!', 'Stop!'],\n",
       "  ['Stop!', 'Arrête-toi !']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.home() / 'data' / 'machine-translation' / 'eng-fra.txt'\n",
    "text = path.read_text().split('\\n')\n",
    "text = [t.replace('\\u202f', '').split('\\t') for t in text]\n",
    "text = [doc for doc in text if len(doc) == 2]\n",
    "len(text), text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08830a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cc4c57c7fb48acb1422afe506ad603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_tokenizer(doc, tokenizer):\n",
    "    return [token.text.lower() for token in tokenizer(doc)]\n",
    "\n",
    "def get_tokens(doc):\n",
    "    tokens_en = apply_tokenizer(doc[0], tokenizer_en)\n",
    "    tokens_fr = apply_tokenizer(doc[1], tokenizer_fr)\n",
    "    return tokens_en, tokens_fr\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "    tokens = [x for x in tqdm(pool.imap(get_tokens, text), total=len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b46bbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105945, 105945)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 10\n",
    "\n",
    "tokens_en = [toks for toks, _ in tokens if len(toks) <= max_tokens and len(_) <= max_tokens]\n",
    "tokens_fr = [toks for _, toks in tokens if len(toks) <= max_tokens and len(_) <= max_tokens]\n",
    "len(tokens_en), len(tokens_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805c17aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['go', '.'], ['va', '!']),\n",
       " (['run', '!'], ['cours', '!']),\n",
       " (['run', '!'], ['courez', '!']),\n",
       " (['wow', '!'], ['ça', 'alors', '!']),\n",
       " (['fire', '!'], ['au', 'feu', '!']),\n",
       " (['help', '!'], ['à', \"l'\", 'aide', '!']),\n",
       " (['jump', '.'], ['saute', '.']),\n",
       " (['stop', '!'], ['ça', 'suffit', '!']),\n",
       " (['stop', '!'], ['stop', '!']),\n",
       " (['stop', '!'], ['arrête', '-', 'toi', '!'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens_en, tokens_fr))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76af0743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_en = torchtext.vocab.build_vocab_from_iterator(tokens_en, specials=['<bos>', '<eos>', '<unk>', '<pad>'],\n",
    "                                                     max_tokens=5000, special_first=True)\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "stoi_en = vocab_en.get_stoi()\n",
    "itos_en = vocab_en.get_itos()\n",
    "\n",
    "vocab_fr = torchtext.vocab.build_vocab_from_iterator(tokens_fr, specials=['<bos>', '<eos>', '<unk>', '<pad>'],\n",
    "                                                     max_tokens=5000, special_first=True)\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n",
    "stoi_fr = vocab_fr.get_stoi()\n",
    "itos_fr = vocab_fr.get_itos()\n",
    "\n",
    "len(vocab_en), len(vocab_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fcae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_en = [vocab_en(toks) for toks in tokens_en]\n",
    "nums_fr = [vocab_fr(toks) for toks in tokens_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50ba4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(nums, seq_len, stoi, bos_token='<bos>', eos_token='<eos>', pad_token='<pad>'):\n",
    "    nums = [stoi[bos_token]] + nums + [stoi[eos_token]]\n",
    "    nums = nums + [stoi[pad_token]] * (seq_len - len(nums))\n",
    "    return nums\n",
    "\n",
    "seq_len = max_tokens + 2 # including <bos>, <eos> around tokens of length 10 already\n",
    "nums_en_padded = [pad_tokens(num, seq_len, stoi_en) for num in nums_en]\n",
    "nums_fr_padded = [pad_tokens(num, seq_len, stoi_fr) for num in nums_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af21b17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 49, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 120, 31, 1, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_en_padded[0], nums_fr_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a17690",
   "metadata": {},
   "source": [
    "# Seq2Seq Models\n",
    "\n",
    "- Learnings a mapping from a sequence $x_1,\\cdots,x_T$ to a new sequence $y_1,\\cdots,y_{T'}$.\n",
    "- The stereotypical application is machine translation, where text in one language is translated to language in another language. For example, translating French `le chat est noir` to English `the cat is black`. \n",
    "- Other examples include named entity recognition (NER), part of speech tagging (POS), or speech recognition (audio to text).\n",
    "- Typically an encoder-decoder architecture is used. The **encoder** is a sequence model (e.g. an RNN) that takes a sequence and outputs a hidden state vector. This, along with the output sequence, then gets fed to a **decoder**, a different sequence model that uses the encoder hidden states and the output sequences to produce a predition sequence.\n",
    "\n",
    "![Seq2Seq](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "**Teacher Forcing**\n",
    "- To help the model learn to predict the output sequence better, a technique called teacher forcing is often used.\n",
    "- One defines some teacher forcing probability $p_{TF}$. When each output prediction $\\hat y_t$ gets made, instead of passing it directly into the input, one *instead* uses the *ground truth* output $y_t$ if $rand() < p_{TF}$.\n",
    "- This can help the model learn to output coherent sequences better, but using it too much can make the model rely on the ground truth outputs, even though at test time such outputs may not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78bea9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_layers, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(input_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x, h = self.rnn(x, h)\n",
    "        return x, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size, n_layers, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(output_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, y, h):       \n",
    "        y = self.emb(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.relu(y)\n",
    "        y, h = self.rnn(y, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, output_size, n_layers=1, bos=0, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, emb_size, hidden_size, n_layers, rnn=rnn, dropout=dropout)\n",
    "        self.decoder = Decoder(output_size, emb_size, hidden_size, n_layers, rnn=rnn, dropout=dropout)\n",
    "        self.output_size = output_size\n",
    "        self.bos = bos\n",
    "    \n",
    "    def forward(self, X, h, Y=None, teacher_forcing=0):\n",
    "        _, h = self.encoder(X, h)\n",
    "        seq_len, batch_size = X.shape[1], X.shape[0]\n",
    "        Y = Y.permute(1, 0) if Y is not None else None\n",
    "        Yhat = torch.empty(seq_len, batch_size, self.output_size)\n",
    "        y_prev = torch.tensor([self.bos] * batch_size).reshape(batch_size, 1).to(device)\n",
    "        for i in range(seq_len):\n",
    "            y_out, h = self.decoder(y_prev, h)\n",
    "            y_prev = y_out.argmax(dim=-1).detach()\n",
    "            y_prev = Y[i][:, None] if np.random.rand() < teacher_forcing else y_prev\n",
    "            Yhat[i] = y_out.permute(1, 0, 2)\n",
    "        Yhat = Yhat.permute(1, 0, 2)\n",
    "        return Yhat.to(device), h.to(device)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.encoder.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17949be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10]),\n",
       " torch.Size([2, 16, 100]),\n",
       " torch.Size([16, 10]),\n",
       " torch.Size([16, 10, 1000]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "batch_size = 16\n",
    "X = torch.randint(0, vocab_size, size=(batch_size,10)).to(device)\n",
    "Y = torch.randint(0, vocab_size, size=(batch_size,10)).to(device)\n",
    "\n",
    "model = Seq2SeqModel(vocab_size, 50, 100, vocab_size, n_layers=2, bos=1).to(device)\n",
    "h = model.init_hidden(batch_size)\n",
    "Yhat, h = model(X, h, Y, teacher_forcing=0.5)\n",
    "X.shape, h.shape, Y.shape, Yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "648abf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9159, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Y.reshape(batch_size * 10,).to(device)\n",
    "yhat = Yhat.reshape(batch_size * 10, -1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c568e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "                  tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "                  print_every=1, n_grad_accums=1, use_multi_gpus=False, grad_clip=None, teacher_forcing=0.5):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    if use_multi_gpus:\n",
    "        orig_model = model\n",
    "        model = nn.DataParallel(model)\n",
    "       \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        # if not specified, use heavy teacher forcing early in training and decay it linearly to zero\n",
    "        teacher_forcing = (1 - epoch / num_epochs) * teacher_forcing\n",
    "        for i, (X, Y) in enumerate(iterator):\n",
    "            X = X.to(device).long()\n",
    "            Y = Y.to(device).long()\n",
    "            bs, bptt = Y.shape[0], Y.shape[1]\n",
    "            h = model.init_hidden(bs)\n",
    "            Yhat, h = model(X, h, Y=Y, teacher_forcing=teacher_forcing)\n",
    "            y = Y.reshape(bs * bptt,)\n",
    "            yhat = Yhat.reshape(bs * bptt, -1)\n",
    "            loss = loss_fn(yhat, y).mean() # mean is for multi-gpu losses, avg them together\n",
    "            loss /= n_grad_accums # rescale loss for grad accumulation\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            if i % n_grad_accums == 0: # only step when every n_grad_accums grad updates\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            opt.zero_grad()\n",
    "            batch_losses = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, Y in iterator:\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "                bs, bptt = Y.shape[0], Y.shape[1]\n",
    "                h = model.init_hidden(bs)\n",
    "                Yhat, h = model(X, h, Y=Y)\n",
    "                y = Y.reshape(bs * bptt,)\n",
    "                yhat = Yhat.reshape(bs * bptt, -1)\n",
    "                loss = loss_fn(yhat, y).mean()\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "        if print_stats and (epoch % print_every == 0) or (epoch == num_epochs - 1):\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   test loss: {round(test_loss, 4): <6}' \n",
    "            print(s1 + s2)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    model = model if not use_multi_gpus else orig_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c6ad2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([95350, 12]),\n",
       " torch.Size([95350, 12]),\n",
       " torch.Size([10595, 12]),\n",
       " torch.Size([10595, 12]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train, idx_test = train_test_split(range(len(nums_en_padded)), train_size=0.9, shuffle=True, random_state=seed)\n",
    "\n",
    "X = torch.tensor(nums_en_padded).long().to(device)\n",
    "Y = torch.tensor(nums_fr_padded).long().to(device)\n",
    "\n",
    "X_train = X[idx_train]\n",
    "Y_train = Y[idx_train]\n",
    "X_test = X[idx_test]\n",
    "Y_test = Y[idx_test]\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2e56c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 40000\n",
    "\n",
    "train_data = Dataset(X_train[:samples], Y_train[:samples])\n",
    "test_data = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c3efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_en = len(vocab_en)\n",
    "vocab_size_fr = len(vocab_fr)\n",
    "emb_size = 256\n",
    "hidden_size = 256\n",
    "bos_out = stoi_fr['<bos>'] # output <bos> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd0c561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6bc909476a4605835f92f5529cd969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     train loss: 5.3814   test loss: 4.7938\n",
      "epoch: 1     train loss: 4.4554   test loss: 4.719 \n",
      "epoch: 2     train loss: 4.3656   test loss: 4.6373\n",
      "epoch: 3     train loss: 4.2953   test loss: 4.6132\n",
      "epoch: 4     train loss: 4.2484   test loss: 4.5877\n",
      "epoch: 5     train loss: 4.2042   test loss: 4.5748\n",
      "epoch: 6     train loss: 4.1269   test loss: 4.3892\n",
      "epoch: 9     train loss: 3.823    test loss: 4.0747\n",
      "epoch: 10    train loss: 3.7534   test loss: 3.9602\n",
      "epoch: 11    train loss: 3.6776   test loss: 3.8682\n",
      "epoch: 12    train loss: 3.6277   test loss: 3.8047\n",
      "epoch: 13    train loss: 3.5571   test loss: 3.7283\n",
      "epoch: 14    train loss: 3.4928   test loss: 3.6622\n",
      "epoch: 15    train loss: 3.4363   test loss: 3.6463\n",
      "epoch: 16    train loss: 3.3769   test loss: 3.545 \n",
      "epoch: 17    train loss: 3.3221   test loss: 3.4915\n",
      "epoch: 18    train loss: 3.2641   test loss: 3.4329\n",
      "epoch: 19    train loss: 3.1938   test loss: 3.376 \n",
      "epoch: 20    train loss: 3.1392   test loss: 3.3419\n",
      "epoch: 21    train loss: 3.1071   test loss: 3.3155\n",
      "epoch: 22    train loss: 3.0799   test loss: 3.2883\n",
      "epoch: 23    train loss: 3.0454   test loss: 3.2663\n",
      "epoch: 24    train loss: 3.0193   test loss: 3.2429\n",
      "epoch: 25    train loss: 2.998    test loss: 3.2199\n",
      "epoch: 26    train loss: 2.9716   test loss: 3.2048\n",
      "epoch: 27    train loss: 2.9354   test loss: 3.1763\n",
      "epoch: 28    train loss: 2.9126   test loss: 3.1583\n",
      "epoch: 29    train loss: 2.8896   test loss: 3.1307\n",
      "epoch: 30    train loss: 2.8605   test loss: 3.1228\n",
      "epoch: 31    train loss: 2.8319   test loss: 3.098 \n",
      "epoch: 32    train loss: 2.8144   test loss: 3.0903\n",
      "epoch: 33    train loss: 2.7852   test loss: 3.0602\n",
      "epoch: 34    train loss: 2.7682   test loss: 3.04  \n",
      "epoch: 35    train loss: 2.7356   test loss: 3.0284\n",
      "epoch: 36    train loss: 2.7109   test loss: 3.0043\n",
      "epoch: 37    train loss: 2.6905   test loss: 2.9934\n",
      "epoch: 38    train loss: 2.6653   test loss: 2.9744\n",
      "epoch: 39    train loss: 2.6475   test loss: 2.9596\n",
      "epoch: 40    train loss: 2.623    test loss: 2.9489\n",
      "epoch: 41    train loss: 2.6108   test loss: 2.9415\n",
      "epoch: 42    train loss: 2.5926   test loss: 2.9306\n",
      "epoch: 43    train loss: 2.5862   test loss: 2.9243\n",
      "epoch: 44    train loss: 2.5744   test loss: 2.916 \n",
      "epoch: 45    train loss: 2.5604   test loss: 2.9141\n",
      "epoch: 46    train loss: 2.5483   test loss: 2.901 \n",
      "epoch: 47    train loss: 2.5416   test loss: 2.901 \n",
      "epoch: 48    train loss: 2.531    test loss: 2.8904\n",
      "epoch: 49    train loss: 2.5218   test loss: 2.8809\n",
      "epoch: 50    train loss: 2.5094   test loss: 2.8766\n",
      "epoch: 51    train loss: 2.5028   test loss: 2.8682\n",
      "epoch: 52    train loss: 2.4891   test loss: 2.8625\n",
      "epoch: 53    train loss: 2.4816   test loss: 2.8541\n",
      "epoch: 54    train loss: 2.4684   test loss: 2.8511\n",
      "epoch: 55    train loss: 2.46     test loss: 2.8439\n",
      "epoch: 56    train loss: 2.4532   test loss: 2.8441\n",
      "epoch: 57    train loss: 2.4372   test loss: 2.8297\n",
      "epoch: 58    train loss: 2.4269   test loss: 2.8251\n",
      "epoch: 59    train loss: 2.4185   test loss: 2.8179\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Seq2SeqModel(vocab_size_en, emb_size, hidden_size, vocab_size_fr, n_layers=1, dropout=0.1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=stoi_en['<pad>']) # ignore loss from <pad> tokens\n",
    "opt = get_optimizer(model, optimizer='adam', lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.5)\n",
    "model = train_seq2seq(train_data, model, opt, loss_fn, num_epochs=60, batch_size=1024, grad_clip=5., print_every=1, \n",
    "                      test_data=test_data, teacher_forcing=0.5, show_batches_bar=False, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c223f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, english):\n",
    "    x = [token.text.lower() for token in tokenizer_en(english)]\n",
    "    x = vocab_en(x)\n",
    "    x = pad_tokens(x, seq_len, vocab_en.get_stoi())\n",
    "    x = torch.tensor(x).long().to(device)[None, :]\n",
    "    h = model.init_hidden(1)\n",
    "    yhat, h = model(x, h)\n",
    "    yhat = yhat.argmax(-1).long().detach().cpu().flatten()\n",
    "    french = [vocab_fr.get_itos()[y] for y in yhat]\n",
    "    return ' '.join(french).replace('<pad>', '').replace('<eos>', '').replace('<bos>', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0550e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qui est - ?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, 'Who are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2634de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), checkpoint)\n",
    "model = Seq2SeqModel(vocab_size_en, emb_size, hidden_size, vocab_size_fr, n_layers=1, dropout=0.1).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea456300",
   "metadata": {},
   "source": [
    "**Image to Text Models**\n",
    "- Maps an image to a sequence of text output. The stereotypical example is image captioning, where one feeds in an image and asks the model to \"describe\" what the image is in words.\n",
    "- Typically a CNN model is used to get a feature map of the image, which is then fed as an input into a sequence model decoder to output text.\n",
    "\n",
    "**Beam Search**\n",
    "- In a seq2seq model, the model tries to learn $p(y_1,\\cdots,y_{T'}|x_1,\\cdots,x_T)$. In many applications, one doesn't want to sample from this distribution, but to find the *most likely* sequence $\\hat y_1, \\cdots, \\hat y_{T'}$, e.g. in machine translation where one wants the *best* possible translation.\n",
    "- The naive way of outputing a prediction is a \"greedy search\". This approach just uses the argmax of each predicted $\\hat y_t$ to make a prediction. Greedy searches aren't generally optimal, in the sense of getting the argmax of the *joint distribution*. In the case of seq2seq, the greedy search can overly focus on the earlier inputs, causing it to miss the big picture.\n",
    "- Beam search is a way around this. It's an approximate algorithm for finding the max of a set of sequences sampled from a joint distribution, i.e. finding $\\hat y = \\text{argmax} p(y_1,\\cdots,y_{T'}|x_1,\\cdots,x_T)$.\n",
    "\n",
    "**Attention Models**\n",
    "- Vanilla seq2seq models tend not to work as well for longer sequences, e.g. 30+ token sequences. This is in essence because they have to remember the entire input to pass into the decoder and decide what the output should be. \n",
    "- Attention models get around this problem by allowing the output to focus just on the local input context (e.g. the first few words) by using a learned \"attention layer\" to learn the probabilities of any one input affecting that output. That way, the decoder can only focus on a specific range of inputs, not the whole sequence.\n",
    "- Example: Consider the translation `Jane visite l'Afrique en septembre` -> `Jane visits Africa in September`. In predicting the output token `Jane`, the model will probably want to look most at the beginning of the input, i.e. `Jane`. So maybe the input tokens should be weighted, e.g. `Jane: 0.9, visite: 0.6, l'Afrique: 0.2, en: 0.1, septembre: 0.1`. That is, the model should focus 90% of its weight on the first word `Jane` when predicting the output word `Jane`.\n",
    "- Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b104c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d9f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e527e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
