{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd8cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import timm\n",
    "import fastai.text.all as fastai\n",
    "from fastai.callback.schedule import Learner\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed + 1)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c2b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bfed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Path.cwd() / 'models' / 'checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c0156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, optimizer='sgd', lr=0.001, weight_decay=0, momentum=0, betas=(0.9, 0.999), eps=1e-8):\n",
    "    if optimizer == 'sgd':\n",
    "        opt = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay, \n",
    "            momentum=momentum\n",
    "        )\n",
    "    if optimizer == 'adam':\n",
    "        opt = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=betas,\n",
    "            eps=eps\n",
    "        )\n",
    "    return opt\n",
    "\n",
    "def train_classifier(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "                     tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "                     print_every=1, n_grad_accums=1, use_multi_gpus=False):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    if use_multi_gpus:\n",
    "        orig_model = model\n",
    "        model = nn.DataParallel(model)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        batch_correct = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        for i, (X, y) in enumerate(iterator):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat, y).mean() # mean is for multi-gpu losses, avg them together\n",
    "            loss /= n_grad_accums # rescale loss for grad accumulation\n",
    "            loss.backward()\n",
    "            if i % n_grad_accums == 0: # only step when every n_grad_accums grad updates\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "            batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        train_acc = sum(batch_correct) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            opt.zero_grad()\n",
    "            batch_losses = []\n",
    "            batch_correct = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, y in iterator:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                yhat = model(X)\n",
    "                loss = loss_fn(yhat, y).mean()\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "                batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "            test_acc = sum(batch_correct) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            test_acc = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Training Accuracy\", train_acc, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Accuracy\", test_acc, epoch+1)\n",
    "        if print_stats and epoch % print_every == 0:\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   train acc: {round(train_acc, 4): <6}   ' \n",
    "            s3 = f'test loss: {round(test_loss, 4): <6}   test acc: {round(test_acc, 4): <6}'\n",
    "            print(s1 + s2 + s3)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    model = model if not use_multi_gpus else orig_model\n",
    "    return model\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119e47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_from_iter(data_iter):\n",
    "#     corpus = []\n",
    "#     for text in data_iter:\n",
    "#         corpus.append(text)\n",
    "#     return corpus\n",
    "\n",
    "# train_iter = torchtext.datasets.WikiText103(split='train')\n",
    "# val_iter = torchtext.datasets.WikiText103(split='valid')\n",
    "# test_iter = torchtext.datasets.WikiText103(split='test')\n",
    "\n",
    "# train_text = text_from_iter(train_iter) + text_from_iter(val_iter)\n",
    "# test_text = text_from_iter(test_iter)\n",
    "\n",
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# train_tokens = (tokenizer(doc) for doc in tqdm(train_text))\n",
    "# test_tokens = (tokenizer(doc) for doc in tqdm(test_text))\n",
    "\n",
    "# vocab = torchtext.vocab.build_vocab_from_iterator(train_tokens, max_tokens=10000, specials=['<unk>', '<pad>'], min_freq=50)\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# stoi = vocab.get_stoi()\n",
    "# itos = vocab.get_itos()\n",
    "\n",
    "# max_tokens = 256\n",
    "# train_tokens = [doc[:max_tokens] for doc in train_tokens if len(doc) > 5]\n",
    "# test_tokens = [doc[:max_tokens] for doc in test_tokens if len(doc) > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5d3e2",
   "metadata": {},
   "source": [
    "# Machine Translation: English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb20639",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "tokenizer_en = torchtext.data.utils.get_tokenizer(nlp_en)\n",
    "tokenizer_fr = torchtext.data.utils.get_tokenizer(nlp_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d529fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135842,\n",
       " [['Go.', 'Va !'],\n",
       "  ['Run!', 'Cours!'],\n",
       "  ['Run!', 'Courez!'],\n",
       "  ['Wow!', 'Ça alors!'],\n",
       "  ['Fire!', 'Au feu !'],\n",
       "  ['Help!', \"À l'aide!\"],\n",
       "  ['Jump.', 'Saute.'],\n",
       "  ['Stop!', 'Ça suffit!'],\n",
       "  ['Stop!', 'Stop!'],\n",
       "  ['Stop!', 'Arrête-toi !']])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.home() / 'data' / 'machine-translation' / 'eng-fra.txt'\n",
    "text = path.read_text().split('\\n')\n",
    "text = [t.replace('\\u202f', '').split('\\t') for t in text]\n",
    "text = [doc for doc in text if len(doc) == 2]\n",
    "len(text), text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08830a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1338cc71a7dc4f3b9310ee23c3c2618e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_tokenizer(doc, tokenizer):\n",
    "    return [token.text.lower() for token in tokenizer(doc)]\n",
    "\n",
    "def get_tokens(doc):\n",
    "    tokens_en = apply_tokenizer(doc[0], tokenizer_en)\n",
    "    tokens_fr = apply_tokenizer(doc[1], tokenizer_fr)\n",
    "    return tokens_en, tokens_fr\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "    tokens = [x for x in tqdm(pool.imap(get_tokens, text), total=len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b46bbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105945, 105945)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 10\n",
    "\n",
    "tokens_en = [toks for toks, _ in tokens if len(toks) <= max_tokens and len(_) <= max_tokens]\n",
    "tokens_fr = [toks for _, toks in tokens if len(toks) <= max_tokens and len(_) <= max_tokens]\n",
    "len(tokens_en), len(tokens_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "805c17aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['go', '.'], ['va', '!']),\n",
       " (['run', '!'], ['cours', '!']),\n",
       " (['run', '!'], ['courez', '!']),\n",
       " (['wow', '!'], ['ça', 'alors', '!']),\n",
       " (['fire', '!'], ['au', 'feu', '!']),\n",
       " (['help', '!'], ['à', \"l'\", 'aide', '!']),\n",
       " (['jump', '.'], ['saute', '.']),\n",
       " (['stop', '!'], ['ça', 'suffit', '!']),\n",
       " (['stop', '!'], ['stop', '!']),\n",
       " (['stop', '!'], ['arrête', '-', 'toi', '!'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens_en, tokens_fr))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76af0743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_en = torchtext.vocab.build_vocab_from_iterator(tokens_en, specials=['<bos>', '<eos>', '<unk>', '<pad>'],\n",
    "                                                     max_tokens=5000, special_first=True)\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "stoi_en = vocab_en.get_stoi()\n",
    "itos_en = vocab_en.get_itos()\n",
    "\n",
    "vocab_fr = torchtext.vocab.build_vocab_from_iterator(tokens_fr, specials=['<bos>', '<eos>', '<unk>', '<pad>'],\n",
    "                                                     max_tokens=5000, special_first=True)\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n",
    "stoi_fr = vocab_fr.get_stoi()\n",
    "itos_fr = vocab_fr.get_itos()\n",
    "\n",
    "len(vocab_en), len(vocab_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fcae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_en = [vocab_en(toks) for toks in tokens_en]\n",
    "nums_fr = [vocab_fr(toks) for toks in tokens_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ba4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(nums, seq_len, stoi, bos_token='<bos>', eos_token='<eos>', pad_token='<pad>'):\n",
    "    nums = [stoi[bos_token]] + nums + [stoi[eos_token]]\n",
    "    nums = nums + [stoi[pad_token]] * (seq_len - len(nums))\n",
    "    return nums\n",
    "\n",
    "seq_len = max_tokens + 2 # including <bos>, <eos> around tokens of length 10 already\n",
    "nums_en_padded = [pad_tokens(num, seq_len, stoi_en) for num in nums_en]\n",
    "nums_fr_padded = [pad_tokens(num, seq_len, stoi_fr) for num in nums_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af21b17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 49, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 120, 31, 1, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_en_padded[0], nums_fr_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a17690",
   "metadata": {},
   "source": [
    "# Seq2Seq Models\n",
    "\n",
    "## Basic Seq2Seq\n",
    "- Learnings a mapping from a sequence $x_1,\\cdots,x_T$ to a new sequence $y_1,\\cdots,y_{T'}$.\n",
    "- The stereotypical application is machine translation, where text in one language is translated to language in another language. For example, translating French `le chat est noir` to English `the cat is black`. \n",
    "- Other examples include named entity recognition (NER), part of speech tagging (POS), or speech recognition (audio to text).\n",
    "- Typically an encoder-decoder architecture is used. The **encoder** is a sequence model (e.g. an RNN) that takes a sequence and outputs a hidden state vector. This, along with the output sequence, then gets fed to a **decoder**, a different sequence model that uses the encoder hidden states and the output sequences to produce a predition sequence.\n",
    "\n",
    "![Seq2Seq](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "**Teacher Forcing**\n",
    "- To help the model learn to predict the output sequence better, a technique called teacher forcing is often used.\n",
    "- One defines some teacher forcing probability $p_{TF}$. When each output prediction $\\hat y_t$ gets made, instead of passing it directly into the input, one *instead* uses the *ground truth* output $y_t$ if $rand() < p_{TF}$.\n",
    "- This can help the model learn to output coherent sequences better, but using it too much can make the model rely on the ground truth outputs, even though at test time such outputs may not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bea9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_layers, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(input_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x, h = self.rnn(x, h)\n",
    "        return x, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size, n_layers, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(output_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, y, h):       \n",
    "        y = self.emb(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.relu(y)\n",
    "        y, h = self.rnn(y, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, output_size, n_layers=1, bos=0, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, emb_size, hidden_size, n_layers, rnn=rnn, dropout=dropout)\n",
    "        self.decoder = Decoder(output_size, emb_size, hidden_size, n_layers, rnn=rnn, dropout=dropout)\n",
    "        self.output_size = output_size\n",
    "        self.bos = bos\n",
    "    \n",
    "    def forward(self, X, h=None, Y=None, teacher_forcing=0):\n",
    "        _, h = self.encoder(X, h)\n",
    "        seq_len, batch_size = X.shape[1], X.shape[0]\n",
    "        Y = Y.permute(1, 0) if Y is not None else None\n",
    "        Yhat = torch.empty(seq_len, batch_size, self.output_size)\n",
    "        y_prev = torch.tensor([self.bos] * batch_size).reshape(batch_size, 1).to(device)\n",
    "        h = h if h is not None else model.init_hidden(batch_size)\n",
    "        for i in range(seq_len):\n",
    "            y_out, h = self.decoder(y_prev, h)\n",
    "            y_prev = y_out.argmax(dim=-1).detach()\n",
    "            y_prev = Y[i][:, None] if np.random.rand() < teacher_forcing else y_prev\n",
    "            Yhat[i] = y_out.permute(1, 0, 2)\n",
    "        Yhat = Yhat.permute(1, 0, 2)\n",
    "        return Yhat.to(device)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.encoder.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17949be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10]),\n",
       " torch.Size([2, 16, 100]),\n",
       " torch.Size([16, 10]),\n",
       " torch.Size([16, 10, 1000]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "batch_size = 16\n",
    "X = torch.randint(0, vocab_size, size=(batch_size,10)).to(device)\n",
    "Y = torch.randint(0, vocab_size, size=(batch_size,10)).to(device)\n",
    "\n",
    "model = Seq2SeqModel(vocab_size, 50, 100, vocab_size, n_layers=2, bos=1).to(device)\n",
    "h = model.init_hidden(batch_size)\n",
    "Yhat = model(X, h, Y, teacher_forcing=0.5)\n",
    "X.shape, h.shape, Y.shape, Yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "648abf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8995, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Y.reshape(batch_size * 10,).to(device)\n",
    "yhat = Yhat.reshape(batch_size * 10, -1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c568e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "                  tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "                  print_every=1, n_grad_accums=1, use_multi_gpus=False, grad_clip=None, teacher_forcing=0.5):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    if use_multi_gpus:\n",
    "        orig_model = model\n",
    "        model = nn.DataParallel(model)\n",
    "       \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        # if not specified, use heavy teacher forcing early in training and decay it linearly to zero\n",
    "        teacher_forcing = (1 - epoch / num_epochs) * teacher_forcing\n",
    "        for i, (X, Y) in enumerate(iterator):\n",
    "            X = X.to(device).long()\n",
    "            Y = Y.to(device).long()\n",
    "            bs, bptt = Y.shape[0], Y.shape[1]\n",
    "            Yhat = model(X, Y=Y, teacher_forcing=teacher_forcing)\n",
    "            y = Y.reshape(bs * bptt,)\n",
    "            yhat = Yhat.reshape(bs * bptt, -1)\n",
    "            loss = loss_fn(yhat, y).mean() # mean is for multi-gpu losses, avg them together\n",
    "            loss /= n_grad_accums # rescale loss for grad accumulation\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            if i % n_grad_accums == 0: # only step when every n_grad_accums grad updates\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            opt.zero_grad()\n",
    "            batch_losses = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, Y in iterator:\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "                bs, bptt = Y.shape[0], Y.shape[1]\n",
    "                Yhat = model(X, Y=Y)\n",
    "                y = Y.reshape(bs * bptt,)\n",
    "                yhat = Yhat.reshape(bs * bptt, -1)\n",
    "                loss = loss_fn(yhat, y).mean()\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "        if print_stats and (epoch % print_every == 0) or (epoch == num_epochs - 1):\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   test loss: {round(test_loss, 4): <6}' \n",
    "            print(s1 + s2)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    model = model if not use_multi_gpus else orig_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6ad2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([95350, 12]),\n",
       " torch.Size([95350, 12]),\n",
       " torch.Size([10595, 12]),\n",
       " torch.Size([10595, 12]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train, idx_test = train_test_split(range(len(nums_en_padded)), train_size=0.9, shuffle=True, random_state=seed)\n",
    "\n",
    "X = torch.tensor(nums_en_padded).long().to(device)\n",
    "Y = torch.tensor(nums_fr_padded).long().to(device)\n",
    "\n",
    "X_train = X[idx_train]\n",
    "Y_train = Y[idx_train]\n",
    "X_test = X[idx_test]\n",
    "Y_test = Y[idx_test]\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e56c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 40000\n",
    "\n",
    "train_data = Dataset(X_train[:samples], Y_train[:samples])\n",
    "test_data = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c3efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_en = len(vocab_en)\n",
    "vocab_size_fr = len(vocab_fr)\n",
    "emb_size = 256\n",
    "hidden_size = 256\n",
    "bos_out = stoi_fr['<bos>'] # output <bos> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd0c561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6bc909476a4605835f92f5529cd969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     train loss: 5.3814   test loss: 4.7938\n",
      "epoch: 1     train loss: 4.4554   test loss: 4.719 \n",
      "epoch: 2     train loss: 4.3656   test loss: 4.6373\n",
      "epoch: 3     train loss: 4.2953   test loss: 4.6132\n",
      "epoch: 4     train loss: 4.2484   test loss: 4.5877\n",
      "epoch: 5     train loss: 4.2042   test loss: 4.5748\n",
      "epoch: 6     train loss: 4.1269   test loss: 4.3892\n",
      "epoch: 9     train loss: 3.823    test loss: 4.0747\n",
      "epoch: 10    train loss: 3.7534   test loss: 3.9602\n",
      "epoch: 11    train loss: 3.6776   test loss: 3.8682\n",
      "epoch: 12    train loss: 3.6277   test loss: 3.8047\n",
      "epoch: 13    train loss: 3.5571   test loss: 3.7283\n",
      "epoch: 14    train loss: 3.4928   test loss: 3.6622\n",
      "epoch: 15    train loss: 3.4363   test loss: 3.6463\n",
      "epoch: 16    train loss: 3.3769   test loss: 3.545 \n",
      "epoch: 17    train loss: 3.3221   test loss: 3.4915\n",
      "epoch: 18    train loss: 3.2641   test loss: 3.4329\n",
      "epoch: 19    train loss: 3.1938   test loss: 3.376 \n",
      "epoch: 20    train loss: 3.1392   test loss: 3.3419\n",
      "epoch: 21    train loss: 3.1071   test loss: 3.3155\n",
      "epoch: 22    train loss: 3.0799   test loss: 3.2883\n",
      "epoch: 23    train loss: 3.0454   test loss: 3.2663\n",
      "epoch: 24    train loss: 3.0193   test loss: 3.2429\n",
      "epoch: 25    train loss: 2.998    test loss: 3.2199\n",
      "epoch: 26    train loss: 2.9716   test loss: 3.2048\n",
      "epoch: 27    train loss: 2.9354   test loss: 3.1763\n",
      "epoch: 28    train loss: 2.9126   test loss: 3.1583\n",
      "epoch: 29    train loss: 2.8896   test loss: 3.1307\n",
      "epoch: 30    train loss: 2.8605   test loss: 3.1228\n",
      "epoch: 31    train loss: 2.8319   test loss: 3.098 \n",
      "epoch: 32    train loss: 2.8144   test loss: 3.0903\n",
      "epoch: 33    train loss: 2.7852   test loss: 3.0602\n",
      "epoch: 34    train loss: 2.7682   test loss: 3.04  \n",
      "epoch: 35    train loss: 2.7356   test loss: 3.0284\n",
      "epoch: 36    train loss: 2.7109   test loss: 3.0043\n",
      "epoch: 37    train loss: 2.6905   test loss: 2.9934\n",
      "epoch: 38    train loss: 2.6653   test loss: 2.9744\n",
      "epoch: 39    train loss: 2.6475   test loss: 2.9596\n",
      "epoch: 40    train loss: 2.623    test loss: 2.9489\n",
      "epoch: 41    train loss: 2.6108   test loss: 2.9415\n",
      "epoch: 42    train loss: 2.5926   test loss: 2.9306\n",
      "epoch: 43    train loss: 2.5862   test loss: 2.9243\n",
      "epoch: 44    train loss: 2.5744   test loss: 2.916 \n",
      "epoch: 45    train loss: 2.5604   test loss: 2.9141\n",
      "epoch: 46    train loss: 2.5483   test loss: 2.901 \n",
      "epoch: 47    train loss: 2.5416   test loss: 2.901 \n",
      "epoch: 48    train loss: 2.531    test loss: 2.8904\n",
      "epoch: 49    train loss: 2.5218   test loss: 2.8809\n",
      "epoch: 50    train loss: 2.5094   test loss: 2.8766\n",
      "epoch: 51    train loss: 2.5028   test loss: 2.8682\n",
      "epoch: 52    train loss: 2.4891   test loss: 2.8625\n",
      "epoch: 53    train loss: 2.4816   test loss: 2.8541\n",
      "epoch: 54    train loss: 2.4684   test loss: 2.8511\n",
      "epoch: 55    train loss: 2.46     test loss: 2.8439\n",
      "epoch: 56    train loss: 2.4532   test loss: 2.8441\n",
      "epoch: 57    train loss: 2.4372   test loss: 2.8297\n",
      "epoch: 58    train loss: 2.4269   test loss: 2.8251\n",
      "epoch: 59    train loss: 2.4185   test loss: 2.8179\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Seq2SeqModel(vocab_size_en, emb_size, hidden_size, vocab_size_fr, n_layers=1, dropout=0.1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=stoi_en['<pad>']) # ignore loss from <pad> tokens\n",
    "opt = get_optimizer(model, optimizer='adam', lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.5)\n",
    "model = train_seq2seq(train_data, model, opt, loss_fn, num_epochs=60, batch_size=1024, grad_clip=5., print_every=1, \n",
    "                      test_data=test_data, teacher_forcing=0.5, show_batches_bar=False, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e605bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkingery/opt/anaconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), checkpoint)\n",
    "model = Seq2SeqModel(vocab_size_en, emb_size, hidden_size, vocab_size_fr, n_layers=1, dropout=0.1).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c223f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, english):\n",
    "    x = [token.text.lower() for token in tokenizer_en(english)]\n",
    "    x = vocab_en(x)\n",
    "    x = pad_tokens(x, seq_len, vocab_en.get_stoi())\n",
    "    x = torch.tensor(x).long().to(device)[None, :]\n",
    "    yhat = model(x)\n",
    "    yhat = yhat.argmax(dim=-1).long().detach().cpu().flatten()\n",
    "    french = [vocab_fr.get_itos()[y] for y in yhat]\n",
    "    return ' '.join(french).replace('<pad>', '').replace('<eos>', '').replace('<bos>', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0550e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qui est -ce ?', 'arrête de !', 'où est -ce ? ?')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'Who are you?'), translate(model, 'Stop!'), translate(model, 'Where am I?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e0bf1",
   "metadata": {},
   "source": [
    "## BLEU Scores\n",
    "- BLEU scores (bilingual evaluation understudy scores) are a metric for scoring machine translation model by matching how many ngrams in the predicted translation overlap with the ngrams in one or more ground truth translations.\n",
    "- For each ngram in the predicted output, the *modified precision* of how many times the ngrams in the prediction appear in each ground truth reference is calculated. The best average of this (against all possible references) over all possible ngrams (up to a specified max n) in the prediction is then returned as the BLEU score.\n",
    "- Modified precision: Suppose a set of $n$-grams (words) $w$ (for a particular $n$) appears in the prediction $\\text{pred}$. Then the modified precision over the set of references is given by the ratio \n",
    "$$\\text{MP}_{n} = \\frac{\\sum_{w \\in \\text{pred}} \\text{# times $w$ appears in all references}}{\\sum_{w \\in \\text{pred}} \\text{# times $w$ appears in prediction}}.$$\n",
    "- The BLEU score is then defined as the exponentiated average over all allowed $n$-grams, weighted by a *brevity length* (BP) factor:\n",
    "$$\\text{BLEU Score} = \\text{BP} \\exp(\\langle \\text{MP}_{n} \\rangle).$$\n",
    "- The BP factor is meant to penalize the model from outputting shorter translations than the ground truth translation. Let $L$ be the max reference output length, and $\\hat L$ be the predicted output length. Then the BP is commonly calculated by\n",
    "$$\\text{BP} = 1 \\text{  if  } \\hat L > L \\text{  else  } \\exp \\bigg(1-\\frac{\\hat L}{L} \\bigg).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9c1887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666865348816"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
    "y = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
    "bleu_score(yhat, y, max_n=1, weights=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61e3cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_to_text(nums, itos, strip_specials=[]):\n",
    "    text = [[itos[idx] for idx in num] for num in nums]\n",
    "    if len(strip_specials) > 0:\n",
    "        text = [[word for word in doc if word not in strip_specials] for doc in text]\n",
    "    return text\n",
    "\n",
    "def text_to_nums(text, stoi, as_tensor=True):\n",
    "    nums = [[stoi[idx] for idx in doc] for doc in text]\n",
    "    if as_tensor:\n",
    "        nums = torch.tensor(nums).long()\n",
    "    return nums\n",
    "\n",
    "def predict(X, model, as_text=False, device=device, batch_size=1024, strip_specials=[]):\n",
    "    if as_text:\n",
    "        X = text_to_nums(X, stoi_en).to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(X, batch_size=batch_size, shuffle=False)\n",
    "    batches = []\n",
    "    for x in tqdm(X):\n",
    "        yhat = model(x[None, :])\n",
    "        batches.append(yhat.detach().argmax(dim=-1).long().to(device))\n",
    "    Yhat = torch.cat(batches, 0)\n",
    "    if as_text:\n",
    "        Yhat = nums_to_text(Yhat.numpy().astype(int).tolist(), itos_fr, strip_specials=strip_specials)\n",
    "    return Yhat\n",
    "\n",
    "def get_bleu_scores(model, Yhat, Y, max_n=1, weights=[1]):\n",
    "    # gotta do this because bleu_score assumes multiple target translations per prediction, not just one\n",
    "    targets = []\n",
    "    preds = []    \n",
    "    for yhat, y in zip(Yhat, Y):        \n",
    "        preds.append(yhat)\n",
    "        targets.append([y])        \n",
    "    return bleu_score(preds, targets, max_n=max_n, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e67b8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa79811faaf14ef197a0ddb259e966da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4692018378644553"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specials = ['<bos>', '<eos>', '<pad>']\n",
    "X_text = nums_to_text(X_test.numpy().tolist(), itos_en, strip_specials=[])\n",
    "Y_text = nums_to_text(Y_test.numpy().tolist(), itos_fr, strip_specials=specials)\n",
    "Yhat_text = predict(X_text, model, as_text=True, strip_specials=specials)\n",
    "get_bleu_scores(model, Yhat_text, Y_text, max_n=1, weights=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "785865a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'will', 'you', 'give', 'me', 'some', 'time', '?', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "['me', '<unk>', '-vous', 'du', 'temps', '?']\n",
      "['me', '<unk>', '-vous', 'tu', 'un', 'un', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "\n",
    "print(X_text[idx])\n",
    "print(Y_text[idx])\n",
    "print(Yhat_text[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bfc7c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af7ad20a263446586723d2dcedc4ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.47646262029039194"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_train = nums_to_text(X_train.numpy().tolist(), itos_en, strip_specials=[])\n",
    "Y_text_train = nums_to_text(Y_train.numpy().tolist(), itos_fr, strip_specials=specials)\n",
    "Yhat_text_train = predict(X_text_train, model, as_text=True, strip_specials=specials)\n",
    "get_bleu_scores(model, Yhat_text_train, Y_text_train, max_n=1, weights=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9bca6",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "- In a sequence prediction model, the model tries to learn a distribution $p(y_1,\\cdots,y_{T'}|x_1,\\cdots,x_T)$. One imagines *sampling* a likely sequence $\\hat y_1, \\cdots, \\hat y_{T'}$ from this joint distribution:\n",
    "$$(\\hat y_1, \\cdots, \\hat y_{T'}) \\sim p(y_1,\\cdots,y_{T'}|x_1,\\cdots,x_T)$$\n",
    "- The naive way of outputing a prediction is a \"greedy search\". This approach just uses the argmax of each predicted $\\hat y_t$ to generate a prediction sequence. This is what is used in training, and most of the time for prediction.\n",
    "- But most of the time for sequence prediction greedy search doesn't generate the *best* sequence as a whole. Rather, it's prone to overly focus on the earlier sequence elements, causing it to optimize early, not focusing on the whole thing at once.\n",
    "- Beam search is a way around this. It's an approximate algorithm for finding the best of a set of sequences sampled from a joint distribution, i.e. approximiately finding the *mode* of the joint distribution.\n",
    "- For seq2seq tasks like machine translation or language modeling, using beam search on the predicted outputs tends to produce better quality translations than naive greedy search (i.e. just using what the model outputs directly).\n",
    "- Formally, beam search is a heuristic, best-first tree-search algorithm. Given $t=1,\\cdots,T'$, output probabilities $p_t \\equiv p(y_t|x_{:t}, y_{:t})$, and a *beam-width* $B$:\n",
    "    - Using $p_0 \\equiv p(y_0|x_0)$, sample the top $B$ classes for $\\hat y_0$: $(\\hat y_0^{(1)},\\cdots,\\hat y_0^{(B)}) \\sim p_0$.\n",
    "    - Using each $p_1^{(b_0)} \\equiv p\\big(y_1|x_0, x_1, \\hat y_0^{(b_0)}\\big)$, sample the top $B$ classes for $\\hat y_1$: $(\\hat y_0^{(b_0,1)},\\cdots,\\hat y_0^{(b_0,B)}) \\sim p_1^{(b_0)}$.\n",
    "    - Repeat recursively for each $t=2,\\cdots,T'$ to get sampled sequences $\\hat y_t^{(b_0,\\cdots,b_{t-1})} \\sim p_t^{(b_0,\\cdots,b_{t-1})} \\equiv p\\big(y_t|x_0, \\cdots, x_t, \\hat y_0^{(b_0)}, \\cdots, \\hat y_{t-1}^{(b_{t-1})}\\big)$.\n",
    "    - Return the sequence of $\\big( \\hat y_t^{(b_0,\\cdots,b_t)} \\big)_{t=1,\\cdots,T'}$ whose branch in the search tree is the maximum product of probabilities, i.e. the (approximate) mode\n",
    "    $$(\\hat y_1, \\cdots, \\hat y_{T'}) \\approx \\text{argmax } \\prod_{t=1}^{T'} p(y_t|x_1,\\cdots,x_{t-1}, y_0,\\cdots, y_{t-1}).$$\n",
    "- A beam width of $B=1$ reduces to regular greedy search. It's the extra \"beams\" that enables beam search to consider other likely and possibly better sequences.\n",
    "- Some refinements to beam search can give improved results:\n",
    "    - Underflow: Due to numerical underflow, the above product can quickly go to zero for moderately long sequences. To get around this, it's common to instead optimize the equivalent negative log likelihood instead:\n",
    "    $$(\\hat y_1, \\cdots, \\hat y_{T'}) \\approx \\text{argmin } \\sum_{t=1}^{T'} -\\log p(y_t|x_1,\\cdots,x_{t-1}, y_0,\\cdots, y_{t-1}).$$\n",
    "    - Length Normalization: For longer sequences, the joint probabilities can be very small. Because of this, beam search tends to prefer shorter sequences. To get around this we can use length normalization. If $J = \\sum_{t=1}^{T'} -\\log p(y_t|x_1,\\cdots,x_{t-1},\\hat y_0,\\cdots,\\hat y_{t-1})$ is the objective function, one can instead optimize instead the scaled objective function for some power $0 \\leq \\alpha \\leq 1$:\n",
    "    $$J' \\equiv \\frac{1}{(T')^\\alpha}J.$$\n",
    "    - Beam Width Tuning: Choosing $B$ to be larger will increase the chance of getting the best sequence, but will also increase compute and memory, and analogously for choosing smaller $B$. Typically, one may choose beam widths in the range $B=10-100$ for industry applications, and even higher for research applications.\n",
    "- As with object detection, where one always applies NMS to model predictions to clean up bounding boxes, with sequence prediction tasks like machine translation, one almost always applies beam search to model predictions to clean up the output quality. Doing error analysis, it can often come down to figuring out whether the model is performing poorly on some examples, or whether beam search is performing poorly. See Andrew Ng's advice video for tips on how to deal with these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "71478685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast pytorch implementation that takes in batches (batch_size, seq_len, vocab_size)\n",
    "# returns sorted tuple of top beam_width indices (beam_width, batch_size, seq_len) along with their beam_width scores\n",
    "def beam_search(probs, beam_width=10, return_type=None):\n",
    "    if not isinstance(probs, torch.Tensor):\n",
    "        probs = torch.tensor(probs)\n",
    "    if len(probs.shape) == 2:\n",
    "        probs = probs[None, :]\n",
    "        \n",
    "    batch_size, seq_len, _ = probs.shape\n",
    "    log_probs = probs.log()\n",
    "    log_prob, indices = log_probs[:, 0, :].topk(beam_width, sorted=True)\n",
    "    indices = indices.unsqueeze(-1)\n",
    "    for i in range(1, seq_len):\n",
    "        log_prob = log_prob.unsqueeze(-1) + log_probs[:, i, :].unsqueeze(1).repeat(1, beam_width, 1)\n",
    "        log_prob, idx = log_prob.reshape(batch_size, -1).topk(beam_width, sorted=True)\n",
    "        indices = torch.cat([indices, idx.unsqueeze(-1)], dim=-1)\n",
    "    \n",
    "    indices = indices.long().cpu()\n",
    "    scores = -log_prob.float().cpu()\n",
    "    if return_type == 'list':\n",
    "        indices = indices.numpy().tolist()\n",
    "        scores = scores.numpy().tolist()\n",
    "    return indices, scores\n",
    "    \n",
    "# very slow python implementation\n",
    "# uses NLL implementation, so mins score = NLL = -log(prob)\n",
    "def beam_search_python(probs, beam_width=10):\n",
    "    seqs = [[[], 0]]\n",
    "    # walk over each step in probs\n",
    "    for prob in tqdm(probs):\n",
    "        all_candidates = []\n",
    "        # expand each current candidate\n",
    "        for i in range(len(seqs)):\n",
    "            seq, score = seqs[i]\n",
    "            for j in range(len(prob)):\n",
    "                candidate = [seq + [j], score - np.log(prob[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda row: row[1], reverse=False)\n",
    "        # select beam_width best [seq, score] pairs by min score\n",
    "        seqs = ordered[:beam_width]\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fd71e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10595, 12, 5000])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yhat = model(X_test)\n",
    "Yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ac2dac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 52, 202, 42, 36, 4, 4, 1, 1, 1, 1, 1], [76, 76, 5202, 5042, 5036, 5004, 5004, 5001, 5001, 5001, 5001, 5001], [52, 123, 10202, 36, 10036, 10004, 10004, 10001, 10001, 10001, 10001, 10001], [123, 45, 177, 5036, 15036, 15004, 15004, 15001, 15001, 15001, 15001, 15001], [18, 5052, 5177, 10042, 20036, 20004, 20004, 20001, 20001, 20001, 20001, 20001], [1461, 5076, 10177, 10036, 25036, 25004, 25004, 25001, 25001, 25001, 25001, 25001], [1646, 5123, 15202, 15042, 42, 30004, 30004, 30001, 30001, 4, 4, 4], [2148, 10052, 20202, 20042, 5042, 35004, 35004, 35001, 35001, 5004, 5004, 5004], [888, 10076, 25202, 15036, 10042, 40004, 40004, 40001, 40001, 10004, 30001, 10004], [516, 10123, 30202, 20036, 15042, 45004, 45004, 45001, 45001, 15004, 10004, 15004]]\n",
      "[[0.0006847356562502682, 0.0006852124934084713, 0.0006861661677248776, 0.0006866430048830807, 0.0006929610972292721, 0.0006943916669115424, 0.0006973719573579729, 0.000697848794516176, 0.0006988024688325822, 0.0006992793059907854]]\n"
     ]
    }
   ],
   "source": [
    "i = 101\n",
    "y = Y_test[i]\n",
    "yhat = Yhat[i].detach().sigmoid().numpy().tolist()\n",
    "best, scores = beam_search(yhat, beam_width=10, return_type='list')\n",
    "print(best[0])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "566ee5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy search:  mon mère m' me . .     \n",
      "beam search:  mon mère m' me . .     \n",
      "ground truth:  ma mère m' a appelé .     \n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(sentence, specials):\n",
    "    for special in specials:\n",
    "        sentence = sentence.replace(special, '')\n",
    "    return sentence\n",
    "\n",
    "greedy_search_sentence = ' '.join([itos_fr[idx] for idx in np.argmax(yhat, axis=1).tolist()])\n",
    "beam_search_sentence = ' '.join([itos_fr[idx] for idx in best[0][0]])\n",
    "ground_truth_sentence = ' '.join([itos_fr[idx] for idx in y])\n",
    "\n",
    "print(f'greedy search: {clean_sentence(greedy_search_sentence, specials)}')\n",
    "print(f'beam search: {clean_sentence(beam_search_sentence, specials)}')\n",
    "print(f'ground truth: {clean_sentence(ground_truth_sentence, specials)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea456300",
   "metadata": {},
   "source": [
    "## Attention\n",
    "- Vanilla seq2seq models tend not to work as well for longer sequences, e.g. 30+ word sequences. This is in essence because they have to remember the entire input to pass into the decoder and decide what the output should be. \n",
    "- Attention models get around this problem by using a learned \"attention layer\" to learn the probabilities of any one input affecting that output. That way, the decoder can only focus on a specific range of inputs (the local context), and not the whole sequence.\n",
    "- Example: Consider the translation `Jane visite l'Afrique en septembre` -> `Jane visits Africa in September`. In predicting the output token `Africa`, the model will probably want to look most around the word `l'Afrique` of the input. So maybe the input tokens should be weighted as below. That is, the model should focus 90% of its weight on the word `l'Afrique` when predicting the output word `Africa`.\n",
    "|Word|Attention Weights - `Africa`|\n",
    "|---|---|\n",
    "|Jane | 0.2 |\n",
    "|visite | 0.6 |\n",
    "|l'Afrique | 0.9 |\n",
    "|en | 0.1 |\n",
    "|septembre | 0.1 |\n",
    "- More formally, consider a seq2seq model with encoder hidden activations $h_1^{(enc)},\\cdots,h_T^{(enc)}$ and decoder activations $h_1^{(dec)},\\cdots,h_{T'}^{(dec)}$. Let $a_{t',t}$ be the attention weights that output $t'$ is paying to input $t$. Define the context vector for $t'$ to be the matrix product $c_{t'} \\equiv a_{t',t} h_{t}^{(enc)}$. Then the input to the decoder will be the elementwise product $y_{t'} \\ast c_{t'}$.\n",
    "- To get the matrix of params $a_{t',t}$, we use another linear layer to compute a set of outputs $e_{t',t}$ given by\n",
    "$$e_{t',t} = W_e [h_{t'-1}^{(dec)}, h_t^{(enc)}] + b_e.$$\n",
    "The attention weights are then given by passing $e_{t',t}$ through a softmax to get probabilities:\n",
    "$$a_{t',t} = \\text{softmax}(e_{t',t}) = \\frac{\\exp(e_{t',t})}{\\sum_t \\exp(e_{t',t})}.$$\n",
    "- Note that attention is quadratic time in the sequence lengths. Since we must calculate each $a_{t',t}$, that gives $O(T'T)$ computations.\n",
    "- It's common to visualize the matrix of attention weights by using a heatmap. This can be used to visually show how much each output $t'$ is influenced by each input $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9b104c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, n_layers, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(input_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(emb_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x, h = self.rnn(x, h)\n",
    "        return x, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size, n_layers, seq_len=seq_len, rnn=nn.GRU, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.emb = nn.Embedding(output_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.attn = nn.Linear(emb_size + hidden_size, seq_len)\n",
    "        self.attn_combine = nn.Linear(emb_size + hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, y, h, enc_out, seq_len=1):       \n",
    "        y = self.emb(y)\n",
    "        y = self.dropout(y)\n",
    "        \n",
    "        h_rep = h.repeat(seq_len, 1, 1).permute(1, 0, 2)\n",
    "        attn = self.attn(torch.cat((y, h_rep), 2)).softmax(dim=2)\n",
    "        c = torch.bmm(attn, enc_out)\n",
    "        yc = torch.cat((y, c), 2)\n",
    "        y = self.attn_combine(yc)\n",
    "        \n",
    "        y = self.relu(y)\n",
    "        y, h = self.rnn(y, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h, attn\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, output_size, n_layers=1, bos=0, rnn=nn.GRU, dropout=0.5,\n",
    "                 seq_len=seq_len, device=device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, emb_size, hidden_size, n_layers, rnn=rnn, dropout=dropout)\n",
    "        self.decoder = AttentionDecoder(output_size, emb_size, hidden_size, n_layers, rnn=rnn, dropout=dropout, \n",
    "                                        seq_len=seq_len)\n",
    "        self.output_size = output_size\n",
    "        self.bos = bos\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, X, h=None, Y=None, teacher_forcing=0):\n",
    "        enc_out, h = self.encoder(X, h)\n",
    "        seq_len, batch_size = X.shape[1], X.shape[0]\n",
    "        Y = Y.permute(1, 0) if Y is not None else None\n",
    "        Yhat = torch.empty(seq_len, batch_size, self.output_size)\n",
    "        y_prev = torch.tensor([self.bos] * batch_size).reshape(batch_size, 1).to(self.device)\n",
    "        h = h if h is not None else model.init_hidden(batch_size)\n",
    "        for i in range(seq_len):\n",
    "            y_out, h, attn = self.decoder(y_prev, h, enc_out, seq_len=1)\n",
    "            y_prev = y_out.argmax(dim=-1).detach()\n",
    "            y_prev = Y[i][:, None] if np.random.rand() < teacher_forcing else y_prev\n",
    "            Yhat[i] = y_out.permute(1, 0, 2)\n",
    "        Yhat = Yhat.permute(1, 0, 2)\n",
    "        return Yhat.to(self.device)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.encoder.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f1305443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12]), torch.Size([128, 12, 1000]))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 1000\n",
    "output_size = 1000\n",
    "emb_size = 50\n",
    "seq_len = seq_len\n",
    "hidden_size = 100\n",
    "batch_size = 128\n",
    "n_layers = 1\n",
    "\n",
    "x = torch.randint(0, input_size, size=(batch_size, seq_len))\n",
    "enc = Encoder(input_size, emb_size, hidden_size, n_layers, rnn=nn.GRU, dropout=0.5)\n",
    "h = enc.init_hidden(batch_size)\n",
    "enc_out, h = enc(x, h)\n",
    "\n",
    "y = torch.randint(0, output_size, size=(batch_size, seq_len)).long()\n",
    "dec = AttentionDecoder(output_size, emb_size, hidden_size, n_layers, seq_len=seq_len, rnn=nn.GRU, dropout=0.5)\n",
    "dec_out, h, a = dec(y, h, enc_out, seq_len=seq_len)\n",
    "\n",
    "model = AttentionModel(input_size, emb_size, hidden_size, output_size, n_layers=1, bos=0, rnn=nn.GRU, dropout=0.5,\n",
    "                       seq_len=seq_len, device=device)\n",
    "yhat = model(x)\n",
    "y.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "309d9f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06694f7337c64650883943d87889fc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     train loss: 5.368    test loss: 4.785 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size_en = len(vocab_en)\n",
    "vocab_size_fr = len(vocab_fr)\n",
    "emb_size = 256\n",
    "hidden_size = 256\n",
    "bos_out = stoi_fr['<bos>'] # output <bos> token\n",
    "\n",
    "model = Seq2SeqModel(vocab_size_en, emb_size, hidden_size, vocab_size_fr, n_layers=1, dropout=0.1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=stoi_en['<pad>']) # ignore loss from <pad> tokens\n",
    "opt = get_optimizer(model, optimizer='adam', lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.5)\n",
    "model = train_seq2seq(train_data, model, opt, loss_fn, num_epochs=60, batch_size=1024, grad_clip=5., print_every=1, \n",
    "                      test_data=test_data, teacher_forcing=0.5, show_batches_bar=False, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), checkpoint)\n",
    "model = Seq2SeqModel(vocab_size_en, emb_size, hidden_size, vocab_size_fr, n_layers=1, dropout=0.1).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d96eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'Who are you?'), translate(model, 'Stop!'), translate(model, 'Where am I?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6217c74",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea6efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff09cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
