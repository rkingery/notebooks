{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f737dec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "\n",
    "import timm\n",
    "import fastai.text.all as fastai\n",
    "from fastai.callback.schedule import Learner\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ac4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d49df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, optimizer='sgd', lr=0.001, weight_decay=0, momentum=0, betas=(0.9, 0.999), eps=1e-8):\n",
    "    if optimizer == 'sgd':\n",
    "        opt = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay, \n",
    "            momentum=momentum\n",
    "        )\n",
    "    if optimizer == 'adam':\n",
    "        opt = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=betas,\n",
    "            eps=eps\n",
    "        )\n",
    "    return opt\n",
    "\n",
    "def train_model(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "               tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "               print_every=1):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        batch_correct = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        for X, y in iterator:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "            batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        train_acc = sum(batch_correct) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            batch_losses = []\n",
    "            batch_correct = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, y in iterator:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                opt.zero_grad()\n",
    "                yhat = model(X)\n",
    "                loss = loss_fn(yhat, y)\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "                batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "            test_acc = sum(batch_correct) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            test_acc = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Training Accuracy\", train_acc, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Accuracy\", test_acc, epoch+1)\n",
    "        if print_stats and epoch % print_every == 0:\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   train acc: {round(train_acc, 4): <6}   ' \n",
    "            s3 = f'test loss: {round(test_loss, 4): <6}   test acc: {round(test_acc, 4): <6}'\n",
    "            print(s1 + s2 + s3)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    return model\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02227860",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- Neural network used to learn input/output relationships for examples that have some kind of sequential, temporal relationship. Allows the network to use learned shared features to take into account the context (sequence order) of a given example.\n",
    "- Maps a temporal sequence $x_1,\\cdots,x_T$ to another temporal sequence $y_1,\\cdots,y_{T'}$ by learning a function of past and present input values: $y_t = f_t(x_1,\\cdots,x_t)$.\n",
    "- Each $x = (x_1,\\cdots,x_T) \\rightarrow y = (y_1,\\cdots,y_{T'})$ pair becomes a \"training example\" in a sense. The output of an RNN layer for a particular sequence element $t=1,\\cdots,T$ is given by\n",
    "$$ h_t = \\tanh(W_{xh} x_t + b_{xh} + W_{hh} h_{t-1} + b_{hh}),$$\n",
    "$$ \\hat y_t = \\sigma(W_{hy} h_t + b_{hy}).$$\n",
    "- One needs to initialize $h_0$ to have some value. One can use zeros, a random initialization, or (for a stateful RNN) use the output $h_0 \\equiv h_T^{prev}$ from the previous batch.\n",
    "- Typically the tanh function is used for hidden activation function in RNNs instead of the ReLU.\n",
    "- Input and output sequences need not have the same length, nor need one or the other be a sequence at all.\n",
    "- Each unit of the RNN has an input state, hidden state, and output state. All params are shared across all elements in the sequence, so only $x_t, h_t$ contain information about position.\n",
    "- Typically $T, T'$ are not actually the length of the input/output sequences, but a defined sequence length. This means each sequence gets split into $k$ chunks of length $T$, potentially breaking the ordering between those chunks.\n",
    "- For sequences of size less than $T,T'$, padding the input/output with `<PAD>` tokens can be used to fill in the gaps.\n",
    "- The standard RNN is only unidirectional. Sequences read left to right. To take advantage of \"acausal\" sequence information, one can use a bidirectional RNN (BiRNN) that uses both left-to-right and right-to-left hidden units.\n",
    "- The loss in an RNN is the sum of the output losses: \n",
    "$$L(y,\\hat y) = L_1(y_1,\\hat y_1) + \\cdots + L_{T'}(y_{T'},\\hat y_{T'}).$$\n",
    "- Backpropagation on an RNN layer is done \"through time\" (BPTT). BPTT works by starting at $y_{T'}$ and backpropagating backwards through time all the way to $x_1$. Since the $W_t$ gradient $\\frac{\\partial L}{\\partial W_t} \\propto W_{t+1} \\dots W_T$, RNNs with long sequence lengths are vulnerable to gradient vanishing. This makes it hard for items in long sequences that are far apart from each other to influence each other.\n",
    "- For sequence classification tasks, e.g. sentiment classification, one has $T'=1$. To use the full context of the sequence, one typically puts the \"useful\" classification vector last in the $x_T$ line. Such RNNs are \"many-to-one\". When $T,T'>1$ the RNN is typically called \"many-to-many\" or \"seq2seq\". When $T=1$ but $T'>1$ the RNN is called \"one-to-many\"; these show up for example in sequence generation tasks.\n",
    "- Encoder-decoder architectures are also possible for many-to-many RNNs. These are used in applications like machine translation. In this scenario, the inputs are fed into an encoder half that outputs nothing. The encoder feeds its hidden units into a decoder that inputs nothing, but outputs the output sequence. Typically in these, the output $\\hat y_{t-1}$ is fed as an effective input into $x_t \\equiv \\hat y_{t-1}$.\n",
    "- In PyTorch, sequences are typically fed into RNNs with the order: `(sequence_length, batch_size, num_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94310636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 hidden_activation=torch.tanh, output_activation=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.linear_in = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.linear_hidden = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.linear_out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        seq_len, batch_size = x.shape[0], x.shape[1]\n",
    "        hidden_shape = (1, batch_size, self.hidden_size)\n",
    "        output_shape = (seq_len, batch_size, self.output_size)\n",
    "        h = torch.zeros(hidden_shape) if h is None else h\n",
    "        yhat = torch.zeros(output_shape)\n",
    "        for t in range(seq_len):\n",
    "            h = self.hidden_activation(self.linear_in(x[t]) + self.linear_hidden(h))\n",
    "            yhat[t] = self.output_activation(self.linear_out(h))\n",
    "        return yhat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89f7855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10]), torch.Size([1, 3, 15]), torch.Size([5, 3, 20]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNNLayer(10, 15, 20, torch.tanh, torch.sigmoid) # input_size, hidden_size, output_size\n",
    "x = torch.randn(5, 3, 10) # seq_len, batch_size, input_size\n",
    "h0 = torch.zeros(1, 3, 15) # num_layers, batch_size, hidden_size\n",
    "yhat, h = rnn(x, h0)\n",
    "x.shape, h.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da70ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10]), torch.Size([1, 3, 20]), torch.Size([5, 3, 20]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 1) # input_size, hidden_size, num_layers\n",
    "x = torch.randn(5, 3, 10) # seq_len, batch_size, input_size\n",
    "h0 = torch.zeros(1, 3, 20) # num_layers, batch_size, hidden_size\n",
    "yhat, h = rnn(x, h0)\n",
    "x.shape, h.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52389d",
   "metadata": {},
   "source": [
    "## Modified RNNs\n",
    "\n",
    "The simple RNN is highly susceptible to gradient vanishing/explosion, making it difficult to learn long range dependencies in sequences. To get around this and enable longer sequences, typically one modifies RNNs slightly by adding \"gates\" to each computation cell, which in reality are just more parameters in the $x_t \\rightarrow y_t$ mappings.\n",
    "\n",
    "**GRUs:** Uses a \"relevance\" gate $r_t$ and an \"update\" gate $u_t$. Informally, the relevance gate decides whether the hidden cell should be updated in response to the previous cell, and the update gate weights how much the new cell is important relative to previous cells via a weighted average.\n",
    "$$\\color{red}{u_t = \\sigma(W_{uu}u_{t-1} + b_{uu} + W_{xu}x_t + b_{xu})}$$\n",
    "$$\\color{red}{r_t = \\sigma(W_{rr}r_{t-1} + b_{rr} + W_{xr}x_t + b_{xr})}$$\n",
    "$$\\tilde h_t = \\tanh(W_{hh}(\\color{red}{r_t \\ast} h_{t-1}) + b_{hh} + W_{xh}x_t + b_{xh})$$\n",
    "$$h_t = \\color{red}{u_t \\ast} \\tilde h_t \\color{red}{+ (1-u_t) \\ast h_{t-1}}$$\n",
    "$$\\hat y_t = \\sigma(W_{hy} h_t + b_{hy}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ab8a125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10]), torch.Size([1, 3, 20]), torch.Size([5, 3, 20]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.GRU(10, 20, 1) # input_size, hidden_size, num_layers\n",
    "x = torch.randn(5, 3, 10) # seq_len, batch_size, input_size\n",
    "h0 = torch.zeros(1, 3, 20) # num_layers, batch_size, hidden_size\n",
    "yhat, h = rnn(x, h0)\n",
    "x.shape, h.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cce73b",
   "metadata": {},
   "source": [
    "**LSTMs:** Uses 3 gates, an \"update\" gate $i_t$, a \"forget\" gate $f_t$, and an \"output\" gate $o_t$. Informally, the update gate decides how important the new hidden cell is, the forget gate decides how much of the previous cell to forget, and the output cell decides how relevant the hidden cell is to the output $h_t$.\n",
    "$$\\color{red}{i_t = \\sigma(W_{ii}i_{t-1} + b_{ii} + W_{xi}x_t + b_{xi})}$$\n",
    "$$\\color{red}{f_t = \\sigma(W_{ff}f_{t-1} + b_{ff} + W_{xf}x_t + b_{xf})}$$\n",
    "$$\\color{red}{o_t = \\sigma(W_{oo}o_{t-1} + b_{oo} + W_{xo}x_t + b_{xo})}$$\n",
    "$$\\tilde c_t = \\tanh(W_{cc} \\color{red}{h_{t-1}} + b_{cc} + W_{xc}x_t + b_{xc})$$\n",
    "$$c_t = \\color{red}{i_t \\ast} \\tilde c_t + \\color{red}{f_t \\ast c_{t-1}}$$\n",
    "$$h_t = \\color{red}{o_t \\ast \\tanh(}c_t\\color{red}{)}$$\n",
    "$$ \\hat y_t = \\sigma(W_{hy} \\color{red}{h_t} + b_{hy}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c6c4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10]),\n",
       " torch.Size([1, 3, 20]),\n",
       " torch.Size([1, 3, 20]),\n",
       " torch.Size([5, 3, 20]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 20, 1) # input_size, hidden_size, num_layers\n",
    "x = torch.randn(5, 3, 10) # seq_len, batch_size, input_size\n",
    "c0 = torch.zeros(1, 3, 20) # num_layers, batch_size, hidden_size\n",
    "h0 = torch.zeros(1, 3, 20) # num_layers, batch_size, hidden_size\n",
    "yhat, (h, c) = rnn(x, (h0, c0))\n",
    "x.shape, h.shape, c.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e8ba0",
   "metadata": {},
   "source": [
    "**Bidirectional RNNs (BiRNNs):** Regular RNNs are \"causal\", meaning $\\hat y_t$ is only a function or *past inputs* $x_1,\\cdots,x_{t-1}$. To allow RNNs to be \"acausal\", one can use BiRNNs, which use hidden activations in both the past and future directions, $h_t$ and $j_t$. One then combines them at the end to get the output predictions.\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} x_t + b_{xh} + W_{hh} h_{t-1} + b_{hh}) \\\\\n",
    "\\color{red}{j_t = \\tanh(W_{xj} x_t + b_{xj} + W_{jj} j_{t+1} + b_{jj})} \\\\\n",
    "\\hat y_t = \\sigma(W_{hy} h_t + b_{hy} \\color{red}{+ W_{j y} j_t + b_{j y}}).\n",
    "$$\n",
    "\n",
    "The downside of a BiRNN is that predictions can't be made in real time. One must wait on the whole sequence to get a prediction even for $t=1$. They are commonly used in NLP, where knowing \"both sides\" of a sentence can make sense when predicting from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56256b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10]),\n",
       " torch.Size([2, 3, 20]),\n",
       " torch.Size([1, 3, 20]),\n",
       " torch.Size([1, 3, 20]),\n",
       " torch.Size([5, 3, 40]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 1, bidirectional=True) # input_size, hidden_size, num_layers\n",
    "x = torch.randn(5, 3, 10) # seq_len, batch_size, input_size\n",
    "hj0 = torch.zeros(2, 3, 20) # 2 * num_layers, batch_size, hidden_size\n",
    "yhat, hj = rnn(x, hj0)\n",
    "h, j = hj[0][None, :], hj[1][None, :]\n",
    "x.shape, hj.shape, h.shape, j.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251f654",
   "metadata": {},
   "source": [
    "**Deep RNNs (DRNNs):** RNNs can be stacked in succession to form layers, as with MLPs. The output of one layer gets fed into the next layer, in succession. Deep RNNs can be useful for more complex sequence prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45178644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10]), torch.Size([2, 3, 20]), torch.Size([5, 3, 40]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.RNN(10, 20, 3) # input_size, hidden_size, num_layers\n",
    "x = torch.randn(5, 3, 10) # seq_len, batch_size, input_size\n",
    "h0 = torch.zeros(2, 3, 20) # num_layers, batch_size, hidden_size\n",
    "yhat, h = rnn(x, h0)\n",
    "x.shape, h.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5925d2",
   "metadata": {},
   "source": [
    "# Text Processing Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9495a220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " 6,\n",
       " [215, 170, 199, 124, 214, 292],\n",
       " ['As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible \\n deniability in order to send them on missions that would otherwise make Gallia lose face in the war',\n",
       "  'While at times \\n this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain \\n members of the 422nd great distress',\n",
       "  'One such member , John , becomes so enraged that he abandons his post and \\n defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , \\n Dahau',\n",
       "  'At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their \\n own interests',\n",
       "  'Feared by both allies and enemies , and combined with the presence of a traitor within their ranks , \\n the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort',\n",
       "  \"This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is \\n escorted to the capital city of Bath in order to present evidence hated the weary soldiers and expose the real \\n traitor , the Gallian General that had accused Kurt of Treason\"])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig = \"\"\"\n",
    " As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible \n",
    " deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times \n",
    " this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain \n",
    " members of the 422nd great distress . One such member , John , becomes so enraged that he abandons his post and \n",
    " defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , \n",
    " Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their \n",
    " own interests . Feared by both allies and enemies , and combined with the presence of a traitor within their ranks , \n",
    " the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . \n",
    " This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is \n",
    " escorted to the capital city of Bath in order to present evidence hated the weary soldiers and expose the real \n",
    " traitor , the Gallian General that had accused Kurt of Treason .\n",
    "\"\"\"\n",
    "\n",
    "corpus = [doc.strip() for doc in orig.split('.') if len(doc.strip()) > 0]\n",
    "labels = [len(doc) for doc in corpus]\n",
    "\n",
    "len(corpus), len(labels), labels, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fd8cd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_special_tokens(text):\n",
    "    # put xxup token before words in all caps (easy way to recognize info from capitalizing a word)\n",
    "    text = re.sub(r'(\\b[A-Z][A-Z0-9]*\\b)', r' xxup \\1 ', text)\n",
    "    # put xxcap token before words with capitalized first letter (easy way to recognize first word in a sentence)\n",
    "    text = re.sub(r'(\\b[A-Z][a-z0-9]+\\b)', r' xxcap \\1 ', text)\n",
    "    # insert beginning and end of sentence tokens xxbos, xxeos after a period, at beginning, and strip extra xxbos\n",
    "    # text = re.sub(r'( [.]+ )', r' xxeos \\1 xxbos ', text)\n",
    "    # text = ' xxbos ' + text\n",
    "    # text = text[:-7]\n",
    "    # insert beginning and end of document tokens xxbod, xxeod\n",
    "    text = ' xxbed ' + text + ' xxeod '\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, remove_stopwords=False, stem_text=False, replace_punct=False):\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    import re, string\n",
    "\n",
    "    # converts common patterns into special tokens\n",
    "    text = sub_special_tokens(text)\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # put spaces between punctuation (eg: 9.Blah -> 9 . Blah)\n",
    "    puncts = r'[' + re.escape(string.punctuation) + r']'\n",
    "    text = re.sub('(?<! )(?=' + puncts + ')|(?<=' + puncts + ')(?! )', r' ', text)\n",
    "    if replace_punct:\n",
    "        # replace all punctuation with xxpunct\n",
    "        text = re.sub(r\"[^\\w\\s]\",' xxpunct ',text)\n",
    "    # convert all other numbers to xxnum token (e.g. 123, 1.2.3, 1-2-3 -> xxnum)\n",
    "    text = re.sub(r'\\b([.-]*[0-9]+[.-]*)+\\b', ' xxnum ', text)\n",
    "    # remove nltk's common set of stop words (common for classical NLP analysis)\n",
    "    if remove_stopwords:\n",
    "        stop_words = stopwords.words('english')\n",
    "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    # stem words using nltk snowball stemmer, e.g. converts {run, running, runs} all to \"run\"\n",
    "    if stem_text:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_text = ''\n",
    "        for word in text.split():\n",
    "                stemmed_text = stemmed_text + stemmer.stem(word) + ' '\n",
    "        text = stemmed_text\n",
    "    # strip new lines\n",
    "    text = re.sub(r'\\n',' ',text)\n",
    "    # sub the occurance of 2 or more spaces with a single space\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "b768d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document in corpus (list of doc strings)\n",
    "# given combined corpus \"word_1 word_2 ...\"\n",
    "# normalize document: \"norm_word_1 norm_word_2 ...\"\n",
    "    # insert relevant special tokens: begin/end of sentence, begin/end of document, caps, all caps, etc\n",
    "# tokenize: get list of tokens [tok_1, tok_2, ...]\n",
    "def tokenize_corpus(corpus, normalizer, tokenizer, *args, **kwargs):\n",
    "    normalized_corpus = [normalizer(doc) for doc in corpus]\n",
    "    tokens = [tokenizer(doc).text.split(' ') for doc in normalized_corpus]\n",
    "    return tokens\n",
    "\n",
    "# vocab: get a dict converting each token to ints\n",
    "# numericalize: use vocab to convert list of tokens to list of ints [int_tok_1, int_tok_2, ...]\n",
    "def numericalize_tokens(tokens, vocab=None, pad_last=True):\n",
    "    if vocab is None:\n",
    "        vocab = list(set([tok for doc in tokens for tok in doc]))\n",
    "        if pad_last:\n",
    "            vocab = vocab + ['xxpad']\n",
    "    vocab_size = len(vocab)\n",
    "    stoi = {vocab[i]: i for i in range(vocab_size)}\n",
    "    itos = {i: vocab[i] for i in range(vocab_size)}\n",
    "    nums = [[stoi[token] for token in doc] for doc in tokens]\n",
    "    return nums, stoi, itos, vocab\n",
    "\n",
    "# batchify numerical tokens into n_batches batches of shape (seq_len, batch_size)\n",
    "    # pad batch items with less than batch_size tokens so all have same length\n",
    "    # items in seqs should preserve token ordering, so tok_1 < tok_2 < ...\n",
    "    # items in batches should preserve sequence ordering, so item_1 < item_2 < ...\n",
    "# return batchified tokens in tuple of tensors of shape (seq_len, batch_size)\n",
    "def sequify(data, seq_len, pad_num, labels=None, flatten=False):\n",
    "    def pad_seqs(seqs_list, pad_num):\n",
    "        for seqs in seqs_list:\n",
    "            leftover_len = len(seqs[-1])\n",
    "            seqs[-1] = seqs[-1] + [pad_num for _ in range(seq_len - leftover_len)]\n",
    "        return seqs_list\n",
    "    def flatten_seq(sequence):\n",
    "            return [seq for doc in sequence for seq in doc]\n",
    "    \n",
    "    seqs = [[x[i:i+seq_len] for i in range(0, len(x), seq_len)] for x in data]\n",
    "    seq_data = pad_seqs(seqs, pad_num)\n",
    "    if labels is not None:\n",
    "        seq_labels = [[labels[i] for seq in seq_data[i]] for i in range(len(seq_data))]\n",
    "        seq_data = flatten_seq(seq_data) if flatten else seq_data\n",
    "        seq_labels = flatten_seq(seq_labels) if flatten else seq_labels\n",
    "        return seq_data, seq_labels\n",
    "    else:\n",
    "        seq_data = flatten_seq(seq_data) if flatten else seq_data\n",
    "        return seq_data    \n",
    "\n",
    "def batchify(seqs, batch_size, labels=None, pad_num=-1):\n",
    "    batchified = [[seq[i: i+batch_size] for i in range(0, len(seq), batch_size)] for seq in seqs]\n",
    "    batched_data = [tuple(torch.tensor(batch).T.long() for batch in batchified[i]) for i in range(len(batchified))]\n",
    "    if labels is not None:\n",
    "        batched_labels = [[[labels[i] for y in x] for x in batchified[i]] for i in range(len(batchified))]\n",
    "        return batched_data, batched_labels\n",
    "    else:\n",
    "        return batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "772cc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_corpus(corpus, normalize_text, nlp)\n",
    "nums, stoi, itos, vocab = numericalize_tokens(tokens, pad_last=True)\n",
    "seq_data, seq_labels = sequify(nums, seq_len=10, pad_num=len(vocab)-1, labels=labels)\n",
    "batched_data, batched_labels = batchify(seq_data, batch_size=3, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a995ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_size = 100\n",
    "\n",
    "class SelectItem(nn.Module):\n",
    "    def __init__(self, idx):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[0][self.idx]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, emb_size),\n",
    "    nn.LSTM(emb_size, 100, 1),\n",
    "    SelectItem(idx=-1),  # selects the last LSTM output y_T to feed into the linear layer\n",
    "    nn.Linear(100, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "cd50dce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[115,  30],\n",
       "         [114,  87],\n",
       "         [  7,  31],\n",
       "         [132,  23],\n",
       "         [ 54,  21],\n",
       "         [134, 136],\n",
       "         [ 50, 136],\n",
       "         [  0, 136],\n",
       "         [129, 136],\n",
       "         [ 18, 136]]),\n",
       " [215, 215, 215],\n",
       " tensor([[-0.0860],\n",
       "         [ 0.0422]]))"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = batched_data[0][-1], batched_labels[0][0]\n",
    "yhat = model(X).detach()\n",
    "X, y, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "3de1647f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'them on missions that would otherwise make xxcap gallia lose face in the war xxeod xxpad xxpad xxpad xxpad xxpad'"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_to_text(batch, itos, batch_first=True):\n",
    "    if isinstance(batch, torch.Tensor):\n",
    "        batch = batch.numpy()\n",
    "    elif isinstance(batch, list):\n",
    "        batch = np.array(batch)\n",
    "    if not batch_first:\n",
    "        batch = batch.T\n",
    "    if len(batch.shape) == 1:\n",
    "        batch = batch[None, :]\n",
    "    return ' '.join(itos[word] for seq in batch for word in seq)\n",
    "\n",
    "batch_to_text(X, itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "1b0e6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sequify(nums, pad_num=len(vocab)-1, seq_len=10, labels=labels, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "dae88e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset(torch.tensor(X).long(), torch.tensor(y).float())\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "5039371a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  5,   0, 131,  31,   0,  92,  72,  80,  62,  12],\n",
       "         [ 90,  31,  82, 105,  96,  31,   0,  35,   0,  83],\n",
       "         [127,  31,  11,  96,  64, 126,  87,  46,  16,  70],\n",
       "         [115, 114,   7, 132,  54, 134,  50,   0, 129,  18],\n",
       "         [ 30,  87,  31,  23,  21, 136, 136, 136, 136, 136],\n",
       "         [  5,   0,   1,  71,  63,  86,  45,  16,  97, 119],\n",
       "         [ 90,  32, 131,  55,  84, 130,  25,   0, 112, 116],\n",
       "         [ 90, 111,  14,  52,  95,  67,  96,  31,  34,  57]]),\n",
       " tensor([215, 215, 215, 215, 215, 170, 170, 170]))"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(data, batch_size=8, shuffle=False)\n",
    "xi, yi = next(iter(data_loader))\n",
    "xi, yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "31f53195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbed xxcap as the xxcap nameless officially do not exist , the upper echelons of the xxcap gallian xxcap army exploit the concept of plausible deniability in order to send'"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_to_text(batched_data[0][0], itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "0b535558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0835],\n",
       "        [-0.0509],\n",
       "        [-0.0532],\n",
       "        [-0.0860],\n",
       "        [ 0.0422],\n",
       "        [-0.1095],\n",
       "        [-0.0509],\n",
       "        [ 0.0303]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xi.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec927e",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "- Uses a sequence of tokens to predict the most likely next tokens, e.g. `I am drinking orange` -> `juice .`\n",
    "- Given a sequence of numericalized tokens:\n",
    "    - Pad the seqs so they all have the same length (often the length of the max seq)\n",
    "    - Set the input $x$ to be the list of sequences (one-hot encoded if no embedding layer)\n",
    "    - Set the output $y$ to be the list of sequences shifted to the right by one (one-hot encoded if no embedding layer)\n",
    "    - The goal is to learn to predict each $y_t|x_t$ where $x_t \\equiv y_{t-1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86bf7da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aachenosaurus',\n",
       " 'Aardonyx',\n",
       " 'Abdallahsaurus',\n",
       " 'Abelisaurus',\n",
       " 'Abrictosaurus',\n",
       " 'Abrosaurus',\n",
       " 'Abydosaurus',\n",
       " 'Acanthopholis',\n",
       " 'Achelousaurus',\n",
       " 'Acheroraptor']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/Users/rkingery/repos/coursera-deep-learning-specialization/C5 - Sequence Models/Week 1/Dinosaur Island -- Character-level language model')\n",
    "df = pd.read_csv(path/'dinos.txt', sep=\" \", header=None)\n",
    "corpus = df.values.flatten().tolist()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32583c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'c', 'h', 'e', 'n', 'o', 's', 'a', 'u', 'r', 'u', 's']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [list(doc.lower()) for doc in corpus]\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4be457f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'e', 'q', 'w', 'p', 's', 'v', 'x', 'c', 'r', 'f', 'z', 'y', 'b', 'k', 'g', 't', 'm', 'j', 'i', 'n', 'o', 'u', 'd', 'h', 'a', '\\n', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(''.join(char for doc in tokens for char in doc)))\n",
    "vocab += ['\\n', '<PAD>']\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b58de2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "stoi = {vocab[i]: i for i in range(vocab_size)}\n",
    "itos = {i: vocab[i] for i in range(vocab_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43932787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(doc) for doc in tokens])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d91c08eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 25, 8, 24, 1, 20, 21, 5, 25, 22, 9, 22, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [[stoi[char] for char in doc] for doc in tokens]\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9017118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 25, 8, 24, 1, 20, 21, 5, 25, 22, 9, 22, 5, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27]\n"
     ]
    }
   ],
   "source": [
    "def pad(seq, pad_token, max_len):\n",
    "    return seq + [pad_token for _ in range(max_len - len(seq))]\n",
    "    \n",
    "seqs_padded = [pad(seq, stoi['<PAD>'], max_len) for seq in sequences]\n",
    "print(seqs_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff1bb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 8, 24, 1, 20, 21, 5, 25, 22, 9, 22, 5, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27]\n"
     ]
    }
   ],
   "source": [
    "seqs_shifted = [seq[1:] + [stoi['<PAD>']] for seq in seqs_padded]\n",
    "print(seqs_shifted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "778b82e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a a c h e n o s a u r u s <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n",
       " 'a c h e n o s a u r u s <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_to_text(seqs_padded[0], itos), batch_to_text(seqs_shifted[0], itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f524062e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536, 26, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_seqs = F.one_hot(torch.tensor(seqs_padded)).float()\n",
    "one_hot_seqs.shape # n_examples, seq_len , vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf023d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536, 26, 28])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_shifted = F.one_hot(torch.tensor(seqs_shifted)).float()\n",
    "one_hot_shifted.shape # n_examples, seq_len , vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0e1d7e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (rnn): RNN(28, 50, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=100, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "seq_len = max_len\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_size, n_layers=1, bidirectional=False, rnn=nn.RNN):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = rnn(vocab_size, hidden_size, n_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        factor = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(factor * hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        a, h = self.rnn(x)\n",
    "        yhat = self.fc(a)\n",
    "        return yhat\n",
    "    \n",
    "model = LanguageModel(vocab_size, 50, vocab_size, n_layers=1, bidirectional=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "054e9880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 26, 28]), torch.Size([1, 26, 28]))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = one_hot_seqs[0][None, :] # 1, seq_len, vocab_size\n",
    "yhat = model(x)\n",
    "x.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "9ab1420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 26, 28]), torch.Size([1536, 26, 28]))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = one_hot_seqs\n",
    "Y = one_hot_shifted\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "112e7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_model(train_data, model, opt, loss_fn, test_data=None, num_epochs=1000, plot_loss=True, \n",
    "                         batch_size=32, print_stats=True, show_batches_bar=False, shuffle=True, max_to_clip=5.,\n",
    "                         print_every=1):\n",
    "    train_size = train_data[0][0].shape[0]\n",
    "    epoch_losses = []\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_to_clip)\n",
    "            opt.step()\n",
    "            batch_losses.append(float(loss) * batch_size)\n",
    "        train_loss = sum(batch_losses) / train_size\n",
    "        epoch_losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            test_size = test_data[0][0].shape[0]\n",
    "            model = model.eval()\n",
    "            batch_losses = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for x, y in iterator:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                opt.zero_grad()\n",
    "                yhat = model(x)\n",
    "                loss = loss_fn(yhat, y)\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "            test_loss = sum(batch_losses) / test_size\n",
    "        else:\n",
    "            test_loss = -999\n",
    "\n",
    "        if print_stats and epoch % print_every == 0:\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   test loss: {round(test_loss, 4): <6}' \n",
    "            print(s1 + s2)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(epoch_losses)), epoch_losses)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "1b868ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113bd6e4e8e140b4baa929979d892a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     train loss: 165.4859   test loss: 152.0459\n",
      "epoch: 50    train loss: 105.9618   test loss: 105.6813\n",
      "epoch: 100   train loss: 103.5779   test loss: 103.284\n",
      "epoch: 150   train loss: 102.6798   test loss: 102.4643\n",
      "epoch: 200   train loss: 102.0696   test loss: 101.8733\n",
      "epoch: 250   train loss: 101.8943   test loss: 101.697\n",
      "epoch: 300   train loss: 101.5912   test loss: 101.4494\n",
      "epoch: 350   train loss: 101.5171   test loss: 101.4886\n",
      "epoch: 400   train loss: 101.4544   test loss: 101.2585\n",
      "epoch: 450   train loss: 101.4821   test loss: 101.3464\n",
      "epoch: 500   train loss: 101.4313   test loss: 101.3246\n",
      "epoch: 550   train loss: 101.3257   test loss: 101.1806\n",
      "epoch: 600   train loss: 101.205   test loss: 100.9912\n",
      "epoch: 650   train loss: 101.078   test loss: 100.9934\n",
      "epoch: 700   train loss: 101.1608   test loss: 101.0752\n",
      "epoch: 750   train loss: 101.0649   test loss: 100.9337\n",
      "epoch: 800   train loss: 101.1896   test loss: 101.0151\n",
      "epoch: 850   train loss: 101.2149   test loss: 101.0049\n",
      "epoch: 900   train loss: 100.954   test loss: 100.9027\n",
      "epoch: 950   train loss: 100.9804   test loss: 100.7855\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = Dataset(X, Y)\n",
    "model = LanguageModel(vocab_size, 50, vocab_size, n_layers=1, rnn=nn.GRU, bidirectional=False).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = get_optimizer(model, optimizer='adam', lr=0.01, weight_decay=0.0001)\n",
    "model = train_language_model(data, model, opt, loss_fn, num_epochs=1000, batch_size=128, max_to_clip=5., \n",
    "                             test_data=data, print_every=1000//20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c6d325e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p a c h y s p o n d y l u s                        \n",
      "h l h e e o o l o l l u r                          \n"
     ]
    }
   ],
   "source": [
    "idx = 1000\n",
    "x = X[idx]\n",
    "y = Y[idx]\n",
    "yhat = model(x)\n",
    "print(batch_to_text(x.argmax(dim=-1), itos).replace('<PAD>', ' '))\n",
    "print(batch_to_text(yhat.argmax(dim=-1), itos).replace('<PAD>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463468de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvements: use LSTMs or GRUs, multiple layers, Adam, LR scheduler, bidirectional RNNs\n",
    "# it's overfitting pretty good when using a GRU with bidirectional\n",
    "# the predictions still look pretty bad...no doubt due to batching and use of bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ee5dca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t y r a n o e p e s u s o s s s <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(model, seed, max_len, output_len):\n",
    "    tokens = pad([stoi[char.lower()] for char in seed], stoi['<PAD>'], max_len)\n",
    "    for i in range(max_len - len(seed)):\n",
    "        x = F.one_hot(torch.tensor(tokens)).float()\n",
    "        yhat = model(x)\n",
    "        probs = yhat.softmax(dim=-1).detach().numpy()[len(seed) + i]\n",
    "        sample = stoi['<PAD>']\n",
    "        if i > output_len:\n",
    "            break\n",
    "        while sample == stoi['<PAD>'] or sample == stoi['\\n']:\n",
    "            sample = np.random.choice(vocab_size, p=probs)\n",
    "        tokens[len(seed) + i] = sample\n",
    "    return tokens\n",
    "\n",
    "seed = 'tyran'\n",
    "tokens = predict(model, seed, max_len, 10)\n",
    "batch_to_text(tokens, itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e0274",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "- Instead of one-hot encoding tokens, it's more efficient to feed their `stoi` index directly into the model via an embedding layer, which is a linear layer that takes an lookup index instead of a vector as input, and outputs a vector of specified size. \n",
    "- Mathematically, an embedding is a learned parameter matrix $E$ applied before the first layer of a neural net: $\\sigma(WEx+b)$. \n",
    "- A given token is identified directly with its embedding vector as a representation of the token in a high-dimensional space. \n",
    "- When trained, these embeddings can learn semantic relationships between tokens, taking into account the context of the token to make predictions. \n",
    "- Embeddings can be used for unsupervised tasks, like similarity search or clustering of tokens and documents.\n",
    "- Embeddings can be used to generate feature maps for text before feeding text into a supervised model. In this sense, embeddings enable transfer learning, where you can get away with training on less data with well-tuned embeddings.\n",
    "- When working with text (or any categorical variable with large numbers of classes), an embedding layer should *always* be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "622a3871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10]), torch.Size([10, 20]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "emb_size = 20\n",
    "\n",
    "x = torch.randint(0, vocab_size, size=(10,))\n",
    "emb = nn.Embedding(vocab_size, emb_size)\n",
    "y = emb(x)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c69b2",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "- Continuous Bag of Words (CBOW): Predict target word given context words\n",
    "- Skip-Gram: Predict some context word given the target word\n",
    "- Model is $\\hat y = \\sigma(WEx+b)$, where $x, y, \\hat y$ all have size $|V| \\times m$.\n",
    "- By default, embeddings have size 300. Vocab sizes can be size 10k or more.\n",
    "- Due to the fact that the vocabulary $|V|$ can be very large, there are a large number of classes, which can make it impractical to calculate the softmax function. To get around this the authors use negative sampling. Instead of solving the original mapping problem, they create an equivalent task as follows:\n",
    "    - For each context-target pair, a number $n$ of negative examples is created to pair with it. The same context is paired with a randomly sampled target, assumed to be false.\n",
    "    - These context-target pairs are then fed into a binary classifier instead of a multiclass classifer, that tries to predict whether the pair is a valid pair or not.\n",
    "- The paper also uses a \"hierarchical softmax\" function instead of the regular softmax, that uses a Huffman Tree to simplify the number of comparisons that need to be done when computing the function. In practice, this isn't really needed anymore with more modern frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39eaf5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/main/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/main/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1805110, 4358)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_from_iter(data_iter):\n",
    "    corpus = []\n",
    "    for text in data_iter:\n",
    "        corpus.append(text)\n",
    "    return corpus\n",
    "\n",
    "train_iter = torchtext.datasets.WikiText103(split='train')\n",
    "val_iter = torchtext.datasets.WikiText103(split='valid')\n",
    "test_iter = torchtext.datasets.WikiText103(split='test')\n",
    "\n",
    "train_text = text_from_iter(train_iter) + text_from_iter(val_iter)\n",
    "test_text = text_from_iter(test_iter)\n",
    "len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb480ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downsampling because this is way too much training data\n",
    "fraction = len(train_text) // 2\n",
    "train_text = train_text[:fraction]\n",
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988ed39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sub_special_tokens(text):\n",
    "#     # put xxup token before words in all caps (easy way to recognize info from capitalizing a word)\n",
    "#     text = re.sub(r'(\\b[A-Z][A-Z0-9]*\\b)', r' xxup \\1 ', text)\n",
    "#     # put xxcap token before words with capitalized first letter (easy way to recognize first word in a sentence)\n",
    "#     text = re.sub(r'(\\b[A-Z][a-z0-9]+\\b)', r' xxcap \\1 ', text)\n",
    "#     # insert beginning and end of sentence tokens xxbos, xxeos after a period, at beginning, and strip extra xxbos\n",
    "#     text = re.sub(r'( [.]+ )', r' xxeos \\1 xxbos ', text)\n",
    "#     text = ' xxbos ' + text\n",
    "#     # text = text[:-7]\n",
    "#     # insert beginning and end of document tokens xxbod, xxeod\n",
    "#     text = ' xxbed ' + text + ' xxeod '\n",
    "#     return text\n",
    "\n",
    "# def normalize_text(text, remove_stopwords=False, stem_text=False, punct=None, lower=True, nums=None, \n",
    "#                    special_tokens=True, strip_non_ascii=False):\n",
    "#     from nltk.stem import SnowballStemmer\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import re, string\n",
    "\n",
    "#     if text.strip() == '':\n",
    "#         return ''\n",
    "#     # strip new lines\n",
    "#     text = re.sub(r'\\n+','',text)\n",
    "#     # put spaces between punctuation (eg: 9.Blah -> 9 . Blah)\n",
    "#     puncts = r'[' + re.escape(string.punctuation) + r']'\n",
    "#     text = re.sub('(?<! )(?=' + puncts + ')|(?<=' + puncts + ')(?! )', r' ', text)\n",
    "#     # converts common patterns into special tokens\n",
    "#     if special_tokens:\n",
    "#         text = sub_special_tokens(text)\n",
    "#     # convert text to lowercase\n",
    "#     if lower:\n",
    "#         text = text.lower()\n",
    "#     # strip non-ascii characters (easy way to denoise text a bit)\n",
    "#     if strip_non_ascii:\n",
    "#         text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "#     # replace all punctuation with xxpunct or strip depending on value of punct\n",
    "#     if punct is not None:\n",
    "#         text = re.sub(r\"[^\\w\\s]\",' xxpunct ', text) if punct == 'replace' else text\n",
    "#         text = re.sub(r\"[^\\w\\s]\",'', text) if punct == 'strip' else text\n",
    "#     # convert all other numbers to xxnum token (e.g. 123, 1.2.3, 1-2-3 -> xxnum)\n",
    "#     if nums is not None:\n",
    "#         text = re.sub(r'\\b([.-]*[0-9]+[.-]*)+\\b', ' xxnum ', text) if nums == 'replace' else text\n",
    "#         text = re.sub(r'\\b([.-]*[0-9]+[.-]*)+\\b', '', text) if nums == 'strip' else text\n",
    "#     # remove nltk's common set of stop words (common for classical NLP analysis)\n",
    "#     if remove_stopwords:\n",
    "#         stop_words = stopwords.words('english')\n",
    "#         text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "#     # stem words using nltk snowball stemmer, e.g. converts {run, running, runs} all to \"run\"\n",
    "#     if stem_text:\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         stemmed_text = ''\n",
    "#         for word in text.split():\n",
    "#                 stemmed_text = stemmed_text + stemmer.stem(word) + ' '\n",
    "#         text = stemmed_text\n",
    "#     # sub the occurance of 2 or more spaces with a single space\n",
    "#     text = re.sub(r'[ ]{2,}',' ',text)\n",
    "#     text = text.strip()\n",
    "#     # fix messed up <unk> tokens\n",
    "#     text = re.sub(r'unk','<unk>',text)       \n",
    "#     return text\n",
    "\n",
    "# def tokenize_corpus(corpus, normalizer, tokenizer, **kwargs):\n",
    "#     tokens = []\n",
    "#     for doc in tqdm(corpus):\n",
    "#         doc = normalizer(doc, **kwargs)\n",
    "#         doc = tokenizer(doc).text\n",
    "#         toks = doc.split(' ') if (doc != '') else ['']\n",
    "#         tokens.append(toks)\n",
    "#     return tokens\n",
    "\n",
    "# tokenizer = torchtext.data.utils.get_tokenizer(nlp) # spacy tokenizer as a function\n",
    "# train_tokens = tokenize_corpus(train_text, normalize_text, tokenizer, punct='strip', nums='strip', strip_non_ascii=True, special_tokens=False)\n",
    "# test_tokens = tokenize_corpus(test_text, normalize_text, tokenizer, punct='strip', nums='strip', strip_non_ascii=True, special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c42109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a96763ec70447308252e2e5a23bbccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/902555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afba9b5a745647639e48faa9fa473a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "train_tokens = (tokenizer(doc) for doc in tqdm(train_text))\n",
    "test_tokens = (tokenizer(doc) for doc in tqdm(test_text))\n",
    "\n",
    "train_tokens = (doc for doc in train_tokens if len(doc) > 10 and len(doc) < 100)\n",
    "test_tokens = (doc for doc in test_tokens if len(doc) > 10 and len(doc) < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc18d8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(train_tokens, max_tokens=10000, specials=['<unk>', '<pad>'], min_freq=50)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "stoi = vocab.get_stoi()\n",
    "itos = vocab.get_itos()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f6bf74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2724169fef64269918a8297576acd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/902555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8c54edba244b5aa2045c5163d84c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "train_tokens = (tokenizer(doc) for doc in tqdm(train_text))\n",
    "test_tokens = (tokenizer(doc) for doc in tqdm(test_text))\n",
    "\n",
    "train_tokens = (doc for doc in train_tokens if len(doc) > 10 and len(doc) < 100)\n",
    "test_tokens = (doc for doc in test_tokens if len(doc) > 10 and len(doc) < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00e7806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a225303be864764a8d41b2af9dc8c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed71dc1695f54e538ed2da755dfbf39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# process text for word2vec CBOW\n",
    "# X = context_window, y = target\n",
    "# ex: 'the quick brown fox jumped' w/ window_size=2\n",
    "# X = the quick _ fox jumped\n",
    "# y = brown\n",
    "\n",
    "def proc_for_word2vec(tokens, window_size=4, pad_token='<pad>'):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for doc in tqdm(tokens):\n",
    "        doc = [pad_token] * window_size + doc + [pad_token] * window_size\n",
    "        for i in range(window_size, len(doc) - window_size):\n",
    "            context = doc[i - window_size: i] + doc[i + 1: i + window_size + 1]\n",
    "            target = doc[i]\n",
    "            contexts.append(context)\n",
    "            targets.append(target)\n",
    "    return contexts, targets\n",
    "\n",
    "window_size = 4\n",
    "train_contexts, train_targets = proc_for_word2vec(train_tokens, window_size=window_size)\n",
    "test_contexts, test_targets = proc_for_word2vec(test_tokens, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57697689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62a73cff5164b88b6217cfbaac6aa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9430149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef30727675c48cc80f5a9013426a526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = [vocab(context) for context in tqdm(train_contexts)]\n",
    "X_test = [vocab(context) for context in tqdm(test_contexts)]\n",
    "\n",
    "y_train = vocab(train_targets)\n",
    "y_test = vocab(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4d0e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9430149, 8]),\n",
       " torch.Size([9430149]),\n",
       " torch.Size([50704, 8]),\n",
       " torch.Size([50704]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train).long().to(device)\n",
    "X_test = torch.tensor(X_test).long().to(device)\n",
    "y_train = torch.tensor(y_train).long().to(device)\n",
    "y_test = torch.tensor(y_test).long().to(device)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ce4c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(X_train, y_train)\n",
    "test_data = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a64a09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch, text_pipeline, window_size, max_len=500):\n",
    "#     batch_input, batch_output = [], []\n",
    "#     for text in batch:\n",
    "#         text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "#         if len(text_tokens_ids) < window_size * 2 + 1:\n",
    "#             continue\n",
    "\n",
    "#         text_tokens_ids = text_tokens_ids[:max_len]\n",
    "#         for idx in range(len(text_tokens_ids) - window_size * 2):\n",
    "#             token_id_sequence = text_tokens_ids[idx : (idx + window_size * 2 + 1)]\n",
    "#             output = token_id_sequence.pop(window_size)\n",
    "#             input = token_id_sequence\n",
    "#             batch_input.append(input)\n",
    "#             batch_output.append(output)\n",
    "#     batch_input = torch.tensor(batch_input).long()\n",
    "#     batch_output = torch.tensor(batch_output).long()\n",
    "#     return batch_input, batch_output\n",
    "\n",
    "\n",
    "# def get_dataloaders_and_vocab(train_iter, val_iter, test_iter, batch_size, window_size=4):\n",
    "#     from functools import partial\n",
    "#     from itertools import chain\n",
    "\n",
    "#     tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "#     vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"], min_freq=50)\n",
    "#     vocab.set_default_index(vocab[\"<unk>\"])        \n",
    "#     text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "#     train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, \n",
    "#                               collate_fn=partial(collate_fn, text_pipeline=text_pipeline))\n",
    "#     val_loader = DataLoader(val_iter, batch_size=batch_size, shuffle=True, \n",
    "#                               collate_fn=partial(collate_fn, text_pipeline=text_pipeline))\n",
    "#     test_loader = DataLoader(test_iter, batch_size=batch_size, shuffle=False, \n",
    "#                               collate_fn=partial(collate_fn, text_pipeline=text_pipeline))    \n",
    "#     return train_loader, val_loader, test_loader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "becdc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter = torchtext.datasets.WikiText103(split='train')\n",
    "# val_iter = torchtext.datasets.WikiText103(split='valid')\n",
    "# test_iter = torchtext.datasets.WikiText103(split='test')\n",
    "\n",
    "# batch_size = 32\n",
    "# window_size = 4\n",
    "# train_loader, val_loader, test_loader, vocab = get_dataloaders_and_vocab(train_iter, val_iter, test_iter, batch_size,\n",
    "#                                                                          window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6d0a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_size = 300\n",
    "\n",
    "class Word2VecCBOW(nn.Module):\n",
    "    def __init__(self, window_size, vocab_size, emb_size=300):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, max_norm=1)\n",
    "        self.fc = nn.Linear(emb_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # pass each x[0, ..., context_window] through self.emb\n",
    "        x = self.emb(x)\n",
    "        # average the context_window outputs together to get an emb_size vector\n",
    "        x = torch.mean(x, dim=1)\n",
    "        # pass vector through linear layer to get vocab_size logits\n",
    "        x = self.fc(x)\n",
    "        # return logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bdc57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2VecCBOW(window_size, vocab_size, emb_size=emb_size).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# opt = get_optimizer(model, optimizer='adam', lr=0.004, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "# model = train_model(train_data, model, opt, loss_fn, num_epochs=100, batch_size=4096, test_data=test_data,\n",
    "#                     scheduler=scheduler, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e65ad81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=4096, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=4096, shuffle=False)\n",
    "dls = fastai.DataLoaders(train_loader, test_loader)\n",
    "\n",
    "model = Word2VecCBOW(window_size, vocab_size, emb_size=emb_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc1de63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.010964781977236271)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, model, loss_func=loss_fn, metrics=fastai.accuracy)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbb035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.00% [1/100 03:22<5:34:32]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.156383</td>\n",
       "      <td>6.075921</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>03:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='2221' class='' max='2303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      96.44% [2221/2303 03:14<00:07 5.9381]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(100, .008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a46918",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Path.cwd()/'models'/'checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model\n",
    "torch.save(model.state_dict(), checkpoint)\n",
    "\n",
    "model = Word2VecCBOW(window_size, vocab_size, emb_size=emb_size).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "learn = Learner(dls, model, loss_func=loss_fn, metrics=fastai.accuracy)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(20, .001)\n",
    "# 4.968465 4.990535 0.204540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9bce15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(20, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8917ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = learn.model\n",
    "# torch.save(model.state_dict(), checkpoint)\n",
    "\n",
    "# model = Word2VecCBOW(window_size, vocab_size, emb_size=emb_size).to(device)\n",
    "# model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "# learn = Learner(dls, model, loss_func=loss_fn, metrics=fastai.accuracy)\n",
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525ce91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15978d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc119f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "embeddings = model.emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdda128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(word, embeddings, stoi):\n",
    "    x = torch.tensor(stoi[word]).long().to(device)\n",
    "    x = embeddings(x)\n",
    "    return x\n",
    "\n",
    "queen = word_embedding('queen', embeddings, stoi)\n",
    "king = word_embedding('king', embeddings, stoi)\n",
    "woman = word_embedding('woman', embeddings, stoi)\n",
    "man = word_embedding('man', embeddings, stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7dbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(king - man + woman, queen, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b348cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_words(word, embeddings, stoi, itos, n_most=10):\n",
    "    if isinstance(word, str):\n",
    "        word = word_embedding(word, embeddings, stoi)[None, :]\n",
    "    embs = list(embeddings.parameters())[0].detach()\n",
    "    sims = F.cosine_similarity(word, embs, dim=1).detach().cpu()\n",
    "    top_n = torch.topk(torch.abs(sims), n_most, largest=False, sorted=True).indices\n",
    "    return [(itos[idx], sims[idx].numpy().item()) for idx in top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_words('paris', embeddings, stoi, itos, n_most=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_words(king - man + woman, embeddings, stoi, itos, n_most=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aecce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d5ab58",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "- Slightly different way of learning word embeddings from word2vec\n",
    "- Given a corpus and vocabular, it uses the co-ocurrance X=(x_{ij}), where x_{ij} is the number of times word j occurs in the context of word i.\n",
    "- Given some weighting function $f(x_{ij})$ it seeks to solve the following optimization problem\n",
    "$$\\text{min} \\sum_{i,j=1}^{|V|} f(x_{ij}) (\\tilde e_j^T e_i + b_i + \\tilde b_j - \\log(x_{ij}))^2$$,\n",
    "where $e, \\tilde e, b, \\tilde b$ are parameter matrices and vectors to be learned\n",
    "- Note $e$ and $\\tilde e$ are both embedding matrices, but aren't exactly equal. To get the final embeddings $E$, just take the average of the two: $E = \\frac{1}{2}(e + \\tilde e)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff98a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974890ae",
   "metadata": {},
   "source": [
    "# Advanced Model Architectures\n",
    "\n",
    "**Seq2Seq Models**\n",
    "- Maps a sequence $x_1,\\cdots,x_T$ to a new sequence $y_1,\\cdots,y_{T'}$.\n",
    "- The stereotypical application is machine translation, where text in one language is translated to language in another language, e.g. `I am going home.` -> `Ich gehe nacht Hause.`. Other examples include named entity recognition (NER), part of speech tagging (POS), or speech recognition (audio to text).\n",
    "- Typically an encoder-decoder architecture is used. The encoder is a sequence model (e.g. an RNN) that takes a sequence and maps it to an intermediate representation, which then gets fed to a decoder, which is a different sequence model that maps representations to the output sequence.\n",
    "\n",
    "**Image to Text Models**\n",
    "- Maps an image to a sequence of text output. The stereotypical example is image captioning, where one feeds in an image and asks the model to \"describe\" what the image is in words.\n",
    "- Typically a CNN model is used to get a feature map of the image, which is then fed as an input into a sequence model decoder to output text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
