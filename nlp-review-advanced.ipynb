{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "\n",
    "import timm\n",
    "import fastai.text.all as fastai\n",
    "from fastai.callback.schedule import Learner\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, optimizer='sgd', lr=0.001, weight_decay=0, momentum=0, betas=(0.9, 0.999), eps=1e-8):\n",
    "    if optimizer == 'sgd':\n",
    "        opt = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay, \n",
    "            momentum=momentum\n",
    "        )\n",
    "    if optimizer == 'adam':\n",
    "        opt = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=betas,\n",
    "            eps=eps\n",
    "        )\n",
    "    return opt\n",
    "\n",
    "def train_classifier(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "                     tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "                     print_every=1, n_grad_accums=1, use_multi_gpus=False):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    if use_multi_gpus:\n",
    "        orig_model = model\n",
    "        model = nn.DataParallel(model)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        batch_correct = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        for i, (X, y) in enumerate(iterator):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat, y).mean() # mean is for multi-gpu losses, avg them together\n",
    "            loss /= n_grad_accums # rescale loss for grad accumulation\n",
    "            loss.backward()\n",
    "            if i % n_grad_accums == 0: # only step when every n_grad_accums grad updates\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "            batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        train_acc = sum(batch_correct) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            opt.zero_grad()\n",
    "            batch_losses = []\n",
    "            batch_correct = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, y in iterator:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                yhat = model(X)\n",
    "                loss = loss_fn(yhat, y).mean()\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "                batch_correct.append(float((yhat.argmax(dim=1) == y).sum().cpu()))\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "            test_acc = sum(batch_correct) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            test_acc = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Training Accuracy\", train_acc, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Accuracy\", test_acc, epoch+1)\n",
    "        if print_stats and epoch % print_every == 0:\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   train acc: {round(train_acc, 4): <6}   ' \n",
    "            s3 = f'test loss: {round(test_loss, 4): <6}   test acc: {round(test_acc, 4): <6}'\n",
    "            print(s1 + s2 + s3)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    model = model if not use_multi_gpus else orig_model\n",
    "    return model\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_from_iter(data_iter):\n",
    "#     corpus = []\n",
    "#     for text in data_iter:\n",
    "#         corpus.append(text)\n",
    "#     return corpus\n",
    "\n",
    "# train_iter = torchtext.datasets.WikiText103(split='train')\n",
    "# val_iter = torchtext.datasets.WikiText103(split='valid')\n",
    "# test_iter = torchtext.datasets.WikiText103(split='test')\n",
    "\n",
    "# train_text = text_from_iter(train_iter) + text_from_iter(val_iter)\n",
    "# test_text = text_from_iter(test_iter)\n",
    "\n",
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# train_tokens = (tokenizer(doc) for doc in tqdm(train_text))\n",
    "# test_tokens = (tokenizer(doc) for doc in tqdm(test_text))\n",
    "\n",
    "# vocab = torchtext.vocab.build_vocab_from_iterator(train_tokens, max_tokens=10000, specials=['<unk>', '<pad>'], min_freq=50)\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# stoi = vocab.get_stoi()\n",
    "# itos = vocab.get_itos()\n",
    "\n",
    "# max_tokens = 256\n",
    "# train_tokens = [doc[:max_tokens] for doc in train_tokens if len(doc) > 5]\n",
    "# test_tokens = [doc[:max_tokens] for doc in test_tokens if len(doc) > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5d3e2",
   "metadata": {},
   "source": [
    "# Machine Translation: English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb20639",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "tokenizer_en = torchtext.data.utils.get_tokenizer(nlp_en)\n",
    "tokenizer_fr = torchtext.data.utils.get_tokenizer(nlp_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d529fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.home() / 'data' / 'machine-translation' / 'eng-fra.txt'\n",
    "text = path.read_text().split('\\n')\n",
    "text = [t.replace('\\u202f', '').split('\\t') for t in text]\n",
    "text = [doc for doc in text if len(doc) == 2]\n",
    "len(text), text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08830a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tokenizer(doc, tokenizer):\n",
    "    return [token.text for token in tokenizer(doc)]\n",
    "\n",
    "def get_tokens(doc):\n",
    "    tokens_en = apply_tokenizer(doc[0], tokenizer_en)\n",
    "    tokens_fr = apply_tokenizer(doc[1], tokenizer_fr)\n",
    "    return tokens_en, tokens_fr\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "    tokens = [x for x in tqdm(pool.imap(get_tokens, text), total=len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10\n",
    "\n",
    "tokens_en = [toks for toks, _ in tokens if len(toks) <= max_tokens and len(_) <= max_tokens]\n",
    "tokens_fr = [toks for _, toks in tokens if len(toks) <= max_tokens and len(_) <= max_tokens]\n",
    "len(tokens_en), len(tokens_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(tokens_en, tokens_fr))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = torchtext.vocab.build_vocab_from_iterator(tokens_en, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "stoi_en = vocab_en.get_stoi()\n",
    "itos_en = vocab_en.get_itos()\n",
    "\n",
    "vocab_fr = torchtext.vocab.build_vocab_from_iterator(tokens_fr, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n",
    "stoi_fr = vocab_fr.get_stoi()\n",
    "itos_fr = vocab_fr.get_itos()\n",
    "\n",
    "len(vocab_en), len(vocab_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_en = [vocab_en(toks) for toks in tokens_en]\n",
    "nums_fr = [vocab_fr(toks) for toks in tokens_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(nums, seq_len, stoi, pad_token='<pad>'):\n",
    "    nums = nums + [stoi[pad_token]] * (seq_len - len(nums))\n",
    "    return nums\n",
    "\n",
    "seq_len = max_tokens\n",
    "nums_en_padded = [pad_tokens(num, seq_len, stoi_en) for num in nums_en]\n",
    "nums_fr_padded = [pad_tokens(num, seq_len, stoi_fr) for num in nums_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_en_padded[0], nums_fr_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a17690",
   "metadata": {},
   "source": [
    "# Seq2Seq Models\n",
    "\n",
    "- Learnings a mapping from a sequence $x_1,\\cdots,x_T$ to a new sequence $y_1,\\cdots,y_{T'}$.\n",
    "- The stereotypical application is machine translation, where text in one language is translated to language in another language. For example, translating French `le chat est noir` to English `the cat is black`. \n",
    "- Other examples include named entity recognition (NER), part of speech tagging (POS), or speech recognition (audio to text).\n",
    "- Typically an encoder-decoder architecture is used. The **encoder** is a sequence model (e.g. an RNN) that takes a sequence and outputs a hidden state vector. This, along with the output sequence, then gets fed to a **decoder**, a different sequence model that uses the encoder hidden states and the output sequences to produce a predition sequence.\n",
    "\n",
    "![Seq2Seq](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bea9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, rnn):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(hidden_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.emb(x)\n",
    "        x = self.relu(x)\n",
    "        x, h = self.rnn(x, h)\n",
    "        return x, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, rnn):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(output_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = rnn(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, y, h):       \n",
    "        y = self.emb(y)\n",
    "        y = self.relu(y)\n",
    "        y, h = self.rnn(y, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, bos, rnn=nn.GRU):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, rnn)\n",
    "        self.decoder = Decoder(output_size, hidden_size, rnn)\n",
    "        self.output_size = output_size\n",
    "        self.bos = bos\n",
    "    \n",
    "    def forward(self, X, h, Y=None):\n",
    "        _, h = self.encoder(X, h)\n",
    "        seq_len, batch_size = X.shape[1], X.shape[0]\n",
    "        Y = Y.permute(1, 0) if Y is not None else None\n",
    "        Yhat = torch.empty(seq_len, batch_size, self.output_size)\n",
    "        y_prev = torch.tensor([self.bos] * batch_size).reshape(batch_size, 1).to(device)\n",
    "        for i in range(seq_len):\n",
    "            y_out, h = self.decoder(y_prev, h)\n",
    "            y_prev = y_out.argmax(dim=-1).detach()\n",
    "            # y_prev = y_prev if ? else Y[i]\n",
    "            Yhat[i] = y_out.permute(1, 0, 2)\n",
    "        Yhat = Yhat.permute(1, 0, 2)\n",
    "        return Yhat, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.encoder.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17949be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "X = torch.randint(0, vocab_size, size=(128,10))\n",
    "Y = torch.randint(0, vocab_size, size=(128,10))\n",
    "\n",
    "model = Seq2SeqModel(vocab_size, 50, vocab_size, bos=1)\n",
    "h = model.init_hidden(128)\n",
    "Yhat, h = model(X, h, Y)\n",
    "X.shape, h.shape, Y.shape, Yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648abf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y.reshape(128 * 10,)\n",
    "yhat = Yhat.reshape(128 * 10, -1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(train_data, model, opt, loss_fn, test_data=None, num_epochs=10, plot_loss=True, batch_size=32,\n",
    "                  tensorboard=False, print_stats=True, show_batches_bar=False, shuffle=True, scheduler=None,\n",
    "                  print_every=1, n_grad_accums=1, use_multi_gpus=False, grad_clip=None):\n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "    losses = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    if test_data is not None:\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    if use_multi_gpus:\n",
    "        orig_model = model\n",
    "        model = nn.DataParallel(model)\n",
    "       \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model = model.train()\n",
    "        batch_losses = []\n",
    "        iterator = tqdm(train_loader, leave=False) if show_batches_bar else train_loader\n",
    "        for i, (X, Y) in enumerate(iterator):\n",
    "            X = X.to(device).long()\n",
    "            Y = Y.to(device).long()\n",
    "            h = model.init_hidden(batch_size)\n",
    "            Yhat, h = model(X, h, Y=Y)\n",
    "            bs, bptt = Y.shape[0], Y.shape[1]\n",
    "            y = Y.reshape(bs * bptt,)\n",
    "            yhat = Yhat.reshape(bs * bptt, -1)\n",
    "            loss = loss_fn(yhat, y).mean() # mean is for multi-gpu losses, avg them together\n",
    "            loss /= n_grad_accums # rescale loss for grad accumulation\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            if i % n_grad_accums == 0: # only step when every n_grad_accums grad updates\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            batch_losses.append(float(loss)* batch_size)\n",
    "        train_loss = sum(batch_losses) / len(train_data)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        if test_data is not None:\n",
    "            model = model.eval()\n",
    "            opt.zero_grad()\n",
    "            batch_losses = []\n",
    "            iterator = tqdm(test_loader, leave=False) if show_batches_bar else test_loader\n",
    "            for X, Y in iterator:\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "                h = model.init_hidden(batch_size)\n",
    "                Yhat, h = model(X, h, Y=Y)\n",
    "                bs, bptt = Y.shape[0], Y.shape[1]\n",
    "                y = Y.reshape(bs * bptt,)\n",
    "                yhat = Yhat.reshape(bs * bptt, -1)\n",
    "                loss = loss_fn(yhat, y).mean()\n",
    "                batch_losses.append(float(loss) * batch_size)\n",
    "            test_loss = sum(batch_losses) / len(test_data)\n",
    "        else:\n",
    "            test_loss = -999\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.add_scalar(\"Training Loss\", train_loss, epoch+1)\n",
    "            writer.add_scalar(\"Test Loss\", test_loss, epoch+1)\n",
    "        if print_stats and epoch % print_every == 0:\n",
    "            s1 = f'epoch: {epoch: <3}   ' \n",
    "            s2 = f'train loss: {round(train_loss, 4): <6}   test loss: {round(test_loss, 4): <6}' \n",
    "            print(s1 + s2)\n",
    "    if plot_loss:\n",
    "        plt.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    model = model if not use_multi_gpus else orig_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ad2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_test = train_test_split(range(len(nums_en_padded)), train_size=0.9, shuffle=True, random_state=seed)\n",
    "\n",
    "X = torch.tensor(nums_en_padded).long()\n",
    "Y = torch.tensor(nums_fr_padded).long()\n",
    "\n",
    "X_train = X[idx_train]\n",
    "Y_train = Y[idx_train]\n",
    "X_test = X[idx_test]\n",
    "Y_test = Y[idx_test]\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e56c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(X_train, Y_train)\n",
    "test_data = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_en = len(vocab_en)\n",
    "vocab_size_fr = len(vocab_fr)\n",
    "emb_size = 100\n",
    "bos = stoi_fr['<bos>'] # output <bos> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(vocab_size_en, emb_size, vocab_size_fr, bos).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = get_optimizer(model, optimizer='adam', lr=0.001, weight_decay=0.)\n",
    "model = train_seq2seq(train_data[:100*128], model, opt, loss_fn, num_epochs=10, batch_size=128, grad_clip=None, \n",
    "                      test_data=test_data[:100*128], print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c223f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokens, vocab_en, vocab_fr):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550e632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634de43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea456300",
   "metadata": {},
   "source": [
    "**Image to Text Models**\n",
    "- Maps an image to a sequence of text output. The stereotypical example is image captioning, where one feeds in an image and asks the model to \"describe\" what the image is in words.\n",
    "- Typically a CNN model is used to get a feature map of the image, which is then fed as an input into a sequence model decoder to output text.\n",
    "\n",
    "**Beam Search**\n",
    "- In a seq2seq model, the model tries to learn $p(y_1,\\cdots,y_{T'}|x_1,\\cdots,x_T)$. In many applications, one doesn't want to sample from this distribution, but to find the *most likely* sequence $\\hat y_1, \\cdots, \\hat y_{T'}$, e.g. in machine translation where one wants the *best* possible translation.\n",
    "- The naive way of outputing a prediction is a \"greedy search\". This approach just uses the argmax of each predicted $\\hat y_t$ to make a prediction. Greedy searches aren't generally optimal, in the sense of getting the argmax of the *joint distribution*. In the case of seq2seq, the greedy search can overly focus on the earlier inputs, causing it to miss the big picture.\n",
    "- Beam search is a way around this. It's an approximate algorithm for finding the max of a set of sequences sampled from a joint distribution, i.e. finding $\\hat y = \\text{argmax} p(y_1,\\cdots,y_{T'}|x_1,\\cdots,x_T)$.\n",
    "\n",
    "**Attention Models**\n",
    "- Vanilla seq2seq models tend not to work as well for longer sequences, e.g. 30+ token sequences. This is in essence because they have to remember the entire input to pass into the decoder and decide what the output should be. \n",
    "- Attention models get around this problem by allowing the output to focus just on the local input context (e.g. the first few words) by using a learned \"attention layer\" to learn the probabilities of any one input affecting that output. That way, the decoder can only focus on a specific range of inputs, not the whole sequence.\n",
    "- Example: Consider the translation `Jane visite l'Afrique en septembre` -> `Jane visits Africa in September`. In predicting the output token `Jane`, the model will probably want to look most at the beginning of the input, i.e. `Jane`. So maybe the input tokens should be weighted, e.g. `Jane: 0.9, visite: 0.6, l'Afrique: 0.2, en: 0.1, septembre: 0.1`. That is, the model should focus 90% of its weight on the first word `Jane` when predicting the output word `Jane`.\n",
    "- Blah\n",
    "\n",
    "**Teacher Forcing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b104c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d9f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e527e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
